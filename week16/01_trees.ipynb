{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees and Forests\n",
    "\n",
    "![](https://cdn.shopify.com/s/files/1/0326/7189/t/65/assets/pf-e820b2e0--mother-tree-forest_1200x.jpg?v=1619557558)\n",
    "\n",
    "မင်္ဂလာပါ။ Week 16 မှ ကြိုဆိုပါတယ်။ ဒီနေ့ ဆွေးနွေးလေ့လာမှာကတော့ Decision Tree နဲ့ Random Forest တို့ပဲ ဖြစ်ပါတယ်။ \n",
    "\n",
    "အောက်မှာ Decision Tree နမူနာတခုကို ပြထားပါတယ်။\n",
    "\n",
    "![](https://forum.huawei.com/enterprise/en/data/attachment/forum/202103/24/190400o09x7rhnnhy2yon7.png?1.png)\n",
    "\n",
    "## Decision Trees\n",
    "\n",
    "Decision Tree တွေဟာ နားလည်ရ၊ ရှင်းပြရလဲ လွယ်ပါတယ်။ \n",
    "\n",
    "အခု စစချင်းမှာ နမူနာနဲ့ စပါမယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, tree, cluster, model_selection, metrics\n",
    "\n",
    "# 1. load the data\n",
    "df_X, ds_y = datasets.load_digits(n_class=2, return_X_y=True, as_frame=True)\n",
    "\n",
    "# 2. split into train and test sets\n",
    "tr_X, ts_X, tr_y, ts_y = model_selection.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "# 3. some feature engineering\n",
    "kmean = cluster.KMeans()\n",
    "feat_tr_X = kmean.fit_transform(tr_X)\n",
    "feat_ts_X = kmean.transform(ts_X)\n",
    "\n",
    "# 4. build tree model\n",
    "tree_model = tree.DecisionTreeClassifier(criterion=\"gini\", splitter=\"best\", max_depth=3, max_features=3, max_leaf_nodes=3, random_state=24)\n",
    "tree_model.fit(feat_tr_X, tr_y)\n",
    "\n",
    "# 5. test the model\n",
    "pred_y = tree_model.predict(feat_ts_X)\n",
    "\n",
    "print (metrics.classification_report(y_true=ts_y, y_pred=pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "စမ်းကြည့်နေတဲ့သူတောင် အံ့သြသွားတယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.plot_tree(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ကဲ ... နောက်ထပ် Regression ပုစ္ဆာတခုကို စမ်းကြည့်ရအောင်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the data\n",
    "df_X, ds_y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "# 2. split into train and test sets\n",
    "tr_X, ts_X, tr_y, ts_y = model_selection.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "# 3. some feature engineering\n",
    "kmean = cluster.KMeans()\n",
    "feat_tr_X1 = kmean.fit_transform(tr_X.values[:, -2:])\n",
    "feat_ts_X1 = kmean.transform(ts_X.values[:, -2:])\n",
    "\n",
    "# 4. more feature engineering \n",
    "# note preprocessing.Normalizer and preprocessing.normalize can't be used \n",
    "# because they have no memory\n",
    "max_values = np.max(tr_X.values[:, :-2], axis=0)\n",
    "feat_tr_X2 = tr_X.values[:, :-2]/max_values\n",
    "feat_ts_X2 = ts_X.values[:, :-2]/max_values\n",
    "\n",
    "# 5. glue them back up\n",
    "feat_tr = np.concatenate((feat_tr_X1, feat_tr_X2), axis=1)\n",
    "feat_ts = np.concatenate((feat_ts_X1, feat_ts_X2), axis=1)\n",
    "\n",
    "# 6. build the model\n",
    "tree_model = tree.DecisionTreeRegressor(criterion=\"squared_error\", max_depth=5, max_features=5, max_leaf_nodes=30, random_state=44)\n",
    "tree_model.fit(feat_tr, tr_y)\n",
    "\n",
    "# 7. test the model\n",
    "pred_y = tree_model.predict(feat_ts)\n",
    "\n",
    "metrics.r2_score(ts_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what to do with this plot too small ?\n",
    "tree.plot_tree(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition behind Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "x = np.linspace(0, 2, 1000)\n",
    "y = stats.norm.pdf(x, loc=0.6, scale=0.3) + np.random.random(1000) * 0.1\n",
    "\n",
    "plt.scatter(x[::10], y[::10], marker=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_draw_linear_model(idx, x, y, idx_start=0, idx_limit=1000):\n",
    "    sample_idx = np.random.choice(idx[idx_start:idx_limit], size=100, replace=False)\n",
    "    # create m as a LinearRegression model and fit using x[sample_idx] and y[sample_idx]\n",
    "    y_pred = m.predict(x.reshape((-1, 1)))\n",
    "    plt.plot(x, y_pred, label=\"model-{}; r2-score: {}\".format(i, metrics.r2_score(y, y_pred)))\n",
    "\n",
    "plt.scatter(x[::10], y[::10], marker=\".\")\n",
    "idx = np.arange(1000)\n",
    "for i in range(5):\n",
    "    build_and_draw_linear_model(idx, x, y, idx_start = 0 if i < 3 else (i - 1) * 150, idx_limit=(i + 1) * 200)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[::5], y[::5], marker=\".\")\n",
    "sample_idx = np.random.choice(idx, size=100, replace=False)\n",
    "tree_model = tree.DecisionTreeRegressor(max_depth=4, max_leaf_nodes=8)\n",
    "tree_model.fit(x[sample_idx].reshape((-1, 1)), y[sample_idx])\n",
    "\n",
    "y_pred = tree_model.predict(x.reshape((-1, 1)))\n",
    "plt.plot(x, y_pred, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print r2-score of the tree_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is too small to see, what to do ?\n",
    "tree.plot_tree(tree_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Decision Tree\n",
    "\n",
    "* Trees tend to overfit\n",
    "* That is (High/Low) Bias and (High/Low) Variance ? (choose correct one)\n",
    "\n",
    "> Try the above problem with various `max_leaf_nodes` and you will see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revisit the Part 1 Project\n",
    "\n",
    "* Try to use `DecisionTreeClassifier` and `KMeans` features for Part 1 Project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "a tree good --> a forest better ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "# 1. load the data\n",
    "df_X, ds_y = datasets.load_digits(return_X_y=True, as_frame=True)\n",
    "\n",
    "# 2. split into train and test sets\n",
    "tr_X, ts_X, tr_y, ts_y = model_selection.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "# 3. some feature engineering\n",
    "kmean = cluster.KMeans()\n",
    "feat_tr_X = kmean.fit_transform(tr_X)\n",
    "feat_ts_X = kmean.transform(ts_X)\n",
    "\n",
    "# 4. build a random forest model\n",
    "forest_model = ensemble.RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=16, max_leaf_nodes=128, n_jobs=4)\n",
    "forest_model.fit(np.concatenate((feat_tr_X, tr_X.values), axis=1), tr_y)\n",
    "\n",
    "# 5. test the model\n",
    "pred_y = forest_model.predict(np.concatenate((feat_ts_X, ts_X.values), axis=1))\n",
    "\n",
    "print (metrics.classification_report(y_true=ts_y, y_pred=pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. load the data\n",
    "df_X, ds_y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "\n",
    "# 2. split into train and test sets\n",
    "tr_X, ts_X, tr_y, ts_y = model_selection.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "# 3. some feature engineering\n",
    "kmean = cluster.KMeans()\n",
    "feat_tr_X1 = kmean.fit_transform(tr_X.values[:, -2:])\n",
    "feat_ts_X1 = kmean.transform(ts_X.values[:, -2:])\n",
    "\n",
    "# 4. more feature engineering \n",
    "# note preprocessing.Normalizer and preprocessing.normalize can't be used \n",
    "# because they have no memory\n",
    "max_values = np.max(tr_X.values[:, :-2], axis=0)\n",
    "feat_tr_X2 = tr_X.values[:, :-2]/max_values\n",
    "feat_ts_X2 = ts_X.values[:, :-2]/max_values\n",
    "\n",
    "# 5. glue them back up\n",
    "feat_tr = np.concatenate((feat_tr_X1, feat_tr_X2), axis=1)\n",
    "feat_ts = np.concatenate((feat_ts_X1, feat_ts_X2), axis=1)\n",
    "\n",
    "# 6. build the model\n",
    "forest_model = ensemble.RandomForestRegressor(criterion=\"squared_error\", max_depth=25, max_features=5, max_leaf_nodes=128, random_state=44, n_jobs=4)\n",
    "forest_model.fit(feat_tr, tr_y)\n",
    "\n",
    "# 7. test the model\n",
    "pred_y = forest_model.predict(feat_ts)\n",
    "\n",
    "metrics.r2_score(ts_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignments**\n",
    "\n",
    "1. Try `load_iris` dataset\n",
    "2. Try `load_wine` dataset\n",
    "3. Try `keras.minst` dataset (`(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()`)\n",
    "\n",
    "4. Try `keras.fashion` dataset (`tf.keras.datasets.fashion_mnist.load_data()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "40960/29515 [=========================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 5s 0us/step\n",
      "26435584/26421880 [==============================] - 5s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "16384/5148 [===============================================================================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 1s 0us/step\n",
      "4431872/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9c1e998c7d6d5f29587b2c70e9bd488bb486b902354401efc27ca8457f04e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
