{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learners\n",
    "\n",
    "မင်္ဂလာပါ။ Data Science Using Python Week 16 မှ ကြိုဆိုပါတယ်။ \n",
    "\n",
    "ဒီနေ့ လေ့လာဆွေးနွေးမဲ့ အကြောင်းအရာက Bayesian Learners တွေပဲ ဖြစ်ပါတယ်။ \n",
    "\n",
    "> တိရစ္ဆာန်ဥယျာဉ် (zoo) တခုမှာ ခြင်္သေ့ ၂ ကောင်၊ ကျား ၂ ကောင်၊ ရေမြင်း ၂ ကောင်၊ သမင် ၄ ကောင်၊ မျောက် ၅ ကောင်နဲ့ ပန်ဒါ ၅ ကောင်ရှိတယ်။\n",
    ">  \n",
    "> ခြင်္သေ့၊ ကျား၊ ရေမြင်းနဲ့ သမင်တွေဟာ ခြေ ၄ ချောင်းနဲ့ လမ်းလျှောက်ပြီး မျောက်နဲ့ ပန်ဒါဟာ ခြေ ၂ ချောင်းနဲ့ လမ်းလျှောက်တယ်။ \n",
    ">\n",
    "> ရေမြင်း၊ သမင်နဲ့ ပန်ဒါတွေဟာ မြက်စားတယ်။\n",
    ">\n",
    "> ၁။ တိရစ္ဆာန်ဥယျာဉ်ထဲက အကောင်တကောင် လွတ်လာတယ်။ အဲဒီ အကောင်ဟာ သမင်ဖြစ်ဖို့ probability ဘယ်လောက် ရှိမလဲ။ What is the p(X=\"Deer\") if we pick up an animal X in random ?\n",
    ">\n",
    "> ၂။ တကယ်လို့ ထွက်လာတဲ့ အကောင်ဟာ ခြေ ၄ ချောင်းနဲ့ လမ်းလျှောက်တယ်ဆိုရင် အဲဒီ အကောင်ဟာ သမင်ဖြစ်ဖို့ probability ဘယ်လောက် ရှိမလဲ။ What is the p(X=\"Deer\") if we know X walks on 4 legs ?\n",
    "> \n",
    "> ၁ နဲ့ ၂ နဲ့ အဖြေတူသလား၊ မတူရင် ဘာလို့ အဖြေပြောင်းတာလဲ။\n",
    "\n",
    "### **Bayesian Learners** တွေ အကြောင်း မိတ်ဆက်\n",
    "\n",
    "* Bayesian Learners တွေဟာ Supervised Machine Learning algorithm တွေ ဖြစ်တယ်။ \n",
    "\n",
    "* သူတို့ဟာ Classification ကိုပဲ လုပ်ဆောင်ပေးနိုင်တယ်။ (Regression မရဘူး)\n",
    "\n",
    "* သူတို့နောက်ကွယ်က သင်္ချာဟာ ရိုးရှင်းပြီး နားလည်ရ လွယ်ကူတယ်။\n",
    "\n",
    "  * အထူးသဖြင့် Naive Bayes တွေပေါ့။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as ds, naive_bayes as nb, model_selection as ms, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ds.fetch_20newsgroups(return_X_y=True, subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "X_test, y_test = ds.fetch_20newsgroups(return_X_y=True, subset=\"test\", remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "data = ds.fetch_20newsgroups(remove=(\"headers\", \"footers\", \"quotes\"))\n",
    "for x_, y_ in zip(X[:10], y[:10]):\n",
    "    print (\"=====================\")\n",
    "    print (x_[:128])\n",
    "    print (\"...\")\n",
    "    print (data.target_names[y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, tname in enumerate(data.target_names):\n",
    "    print (idx, tname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take (7, 8, 9) as class 0 (autos), (1, 2, 3, 4, 5) as class 1 (computers) and the rest as class 2 (others)\n",
    "import numpy as np \n",
    "@np.vectorize\n",
    "def get_target(y):\n",
    "    if y in [7, 8, 9]:\n",
    "        return 0\n",
    "    elif y in [1, 2, 3, 4, 5]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "target = get_target(y)\n",
    "target_test = get_target(y_test)\n",
    "target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text as text_fe\n",
    "\n",
    "tfidf_transformer = text_fe.TfidfVectorizer(input=\"content\", use_idf=True, norm=\"l2\", stop_words=\"english\")\n",
    "X_ = tfidf_transformer.fit_transform(X[:10])\n",
    "feature_words = tfidf_transformer.get_feature_names_out()\n",
    "feature_selector = [idx for idx, word in enumerate(feature_words) if \"A\" <= word[0].upper() <= \"Z\"]\n",
    "print (feature_words[feature_selector])\n",
    "X_[:10, feature_selector].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = text_fe.TfidfVectorizer(input=\"content\", use_idf=True, norm=\"l2\", stop_words=\"english\")\n",
    "X_ = tfidf_transformer.fit_transform(X)\n",
    "feature_words = tfidf_transformer.get_feature_names_out()\n",
    "feature_selector = [idx for idx, word in enumerate(feature_words) if \"A\" <= word[0].upper() <= \"Z\"]\n",
    "X_test_ = tfidf_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.MultinomialNB(alpha=.01)\n",
    "model.fit(X_[:, feature_selector], target)\n",
    "pred = model.predict(X_test_[:, feature_selector])\n",
    "print (metrics.classification_report(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_b = nb.BernoulliNB()\n",
    "model_b.fit(X_[:, feature_selector].todense(), target)\n",
    "pred = model_b.predict(X_test_[:, feature_selector].todense())\n",
    "print (metrics.classification_report(target_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait! What ? \n",
    "\n",
    "**Computer can read English ??? Black Magic ???**\n",
    "\n",
    "Short answer: No. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfIdf\n",
    "\n",
    "TfIdf stands for \"Term Frequency - inverse Document Frequency\".\n",
    "\n",
    "TfIdf မှာ 2 ပိုင်းပါတယ်။ \n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "စဉ်းစားကြည့်ရအောင်။ ဒီအောက်က စာပိုဒ်က rec.autos (ကား/ဆိုင်ကယ်အကြောင်း) ဖြစ်မလား။ comp.* (ကွန်ပျူတာအကြောင်း) ဖြစ်မလား။ \n",
    "\n",
    "> 'I was wondering if anyone out there could enlighten me on this **car** I saw the other day. It was a **2-door sports car**, looked to be from the **late 60s nearly 70s**. It was called a **Bricklin**. The **doors** were really small. In addition, the **front bumper** was separate from the rest of the **body**. This is all I know. If anyone can tellme a **model name, engine specs, years of production**, where this **car** is **made**, history, or whatever info you have on this funky looking **car**, please e-mail.'\n",
    "\n",
    "နောက်တခု ... ဒီအောက်က စာပိုဒ်က rec.autos (ကား/ဆိုင်ကယ်အကြောင်း) ဖြစ်မလား။ comp.* (ကွန်ပျူတာအကြောင်း) ဖြစ်မလား။ \n",
    "\n",
    "> \"A fair number of brave souls who upgraded their SI **clock** oscillator have shared their experiences for this poll. Please send a brief message detailing your experiences with the **procedure**. Top speed attained, **CPU** rated speed,add on **cards and adapters**, **heat sinks, hour of usage per day, floppy disk** functionality with 800 and 1.4 m **floppies** are especially requested. I will be summarizing in the next two days, so please add to the network knowledge base if you have done the **clock upgrade** and haven't answered this poll. Thanks.\"\n",
    "\n",
    "**ဘယ်လို သိသလဲ။**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_1 = X[0]\n",
    "text_2 = X[1]\n",
    "\n",
    "tfidf_trans = text_fe.CountVectorizer(input=\"content\", stop_words=\"english\", max_features=20)\n",
    "features = tfidf_trans.fit_transform(X[0:2])\n",
    "df = pd.DataFrame(data=features.todense(), columns=tfidf_trans.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီနေရာမှာ issue 2 ခု ပေါ်လာတယ်။ \n",
    "\n",
    "1. ပထမတခုက vector ၂ ခုကြားထဲက distance ကို ၂ မျိုး တွက်လို့ရတယ်။ ပထမနည်းက သိပြီးဖြစ်တဲ့ Euclidean Distance (vector ခေါင်း မြှားထိပ် ၂ ခုရဲ့ အကွာအဝေးကို တွက်တာ)။ ဒုတိယနည်းက vector ၂ ခုကြားထဲက ထောင့် (angle) ကို တွက်တာ။ cosine distance (1 - cosine similarity) လို့ ခေါ်တယ်။ \n",
    "\n",
    "    text document တွေအတွက်က cosine distance က ပိုပြီး အဓိပ္ပါယ်ရှိတယ်။ cosine distance မှာ vector ရဲ့ length က အရေးမပါဘူး (lenght ပေါ်မူတည်ပြီး ပြောင်းမသွားဘူး)။ ဒါကြောင့် unit vector ဖြစ်အောင် Normalize လုပ်လိုက်လဲ အဖြေတူတူပဲ ဖြစ်တယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "n = Normalizer()\n",
    "pd.DataFrame(n.fit_transform(df), columns=tfidf_trans.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> တဆင့်တည်းနဲ့ ပြီးစေဖို့ အလွယ်နည်းနဲ့ `norm=\"l2\"` ဆိုပြီး `TfidfVectorizer` မှာ ပေးလိုက်လို့ ရတယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = X[0]\n",
    "text_2 = X[1]\n",
    "\n",
    "tfidf_trans = text_fe.TfidfVectorizer(input=\"content\", use_idf=False, norm=\"l2\", stop_words=\"english\", max_features=20)\n",
    "features = tfidf_trans.fit_transform(X[0:2])\n",
    "pd.DataFrame(data=features.todense(), columns=tfidf_trans.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ဒုတိယတခုက တချို့စာလုံးတွေ (\"the\", \"I\", \"thanks\", \"message\") လိုဟာတွေက topic ၂ ခုလုံးမှာ ပါနေကြတယ်။ ဒါကြောင့် ဒါတွေကို discount လုပ်ပစ်ဖို့ **Inverse Document Frequency** ကို သုံးရတယ်။\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency\n",
    "\n",
    "စာလုံး တလုံးချင်းစီရဲ့ frequency ကို သူတို့ပါတဲ့ document အရေအတွက်နဲ့ စားပစ်လိုက်တယ်။ ဥပမာ \"the\" ဆိုရင် document အားလုံးနီးပါးမှာပါတယ်။ \"car\" ဆိုရင် ကားနဲ့ ပတ်သက်တဲ့ document တွေမှာပဲ ပါတယ်။ ဒီတော့ \"the\" ရဲ့ Document Frequency က 1,000,000 ဖြစ်ပေမဲ့ \"car\" ရဲ့ Document Frequency က 100 ဖြစ်ချင်ဖြစ်မယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = X[0]\n",
    "text_2 = X[1]\n",
    "\n",
    "tfidf_trans = text_fe.TfidfVectorizer(input=\"content\", use_idf=True, norm=\"l2\", stop_words=\"english\", max_features=20)\n",
    "features = tfidf_trans.fit_transform(X[0:2])\n",
    "pd.DataFrame(data=features.todense(), columns=tfidf_trans.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီနည်းနဲ့ Text တွေပါတဲ့ data `X` ကို sklearn က လက်ခံနိုင်တဲ့ 2D matrix `X_` ပုံစံကို ပြောင်းလို့ရသွားတယ်။\n",
    "\n",
    "ဒီတော့ Bayesian Learner တွေသာမက `KMeans` လိုမျိုး Clustering တွေအတွက်ပါ သုံးလို့ရသွားတယ်။ \n",
    "\n",
    "> **KEY TAKEAWAY** Unit Vector ဖြစ်အောင် Normalize လုပ်ထားတဲ့ Data အတွက် Eucliden Distance နဲ့ Cosine Distance က အတူတူပဲ ဖြစ်တယ်။ Interview Question !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "တိရစ္ဆာန်ဥယျာဥ်အကြောင်း ပြန်စဉ်းစားကြစို့။\n",
    "\n",
    "> တိရစ္ဆာန်ဥယျာဉ် (zoo) တခုမှာ ခြင်္သေ့ ၂ ကောင်၊ ကျား ၂ ကောင်၊ ရေမြင်း ၂ ကောင်၊ သမင် ၄ ကောင်၊ မျောက် ၅ ကောင်နဲ့ ပန်ဒါ ၅ ကောင်ရှိတယ်။\n",
    ">  \n",
    "> ခြင်္သေ့၊ ကျား၊ ရေမြင်းနဲ့ သမင်တွေဟာ ခြေ ၄ ချောင်းနဲ့ လမ်းလျှောက်ပြီး မျောက်နဲ့ ပန်ဒါဟာ ခြေ ၂ ချောင်းနဲ့ လမ်းလျှောက်တယ်။ \n",
    ">\n",
    "> ရေမြင်း၊ သမင်နဲ့ ပန်ဒါတွေဟာ မြက်စားတယ်။\n",
    "\n",
    "မြက်စားဖို့ Probability, \n",
    "\n",
    "$$P(\\text{eat grass})=\\frac{2+4+5}{20} = \\frac{11}{20}$$ \n",
    "\n",
    "ရှိတယ်။\n",
    "\n",
    "ဒီလိုပဲ ... ခြေ ၄ ချောင်းနဲ့ လမ်းလျှောက်ဖို့ Probability, \n",
    "\n",
    "$$P(\\text{walk on 4})=\\frac{2+2+2+4}{20}=\\frac{10}{20}$$\n",
    "\n",
    "ရှိတယ်။ \n",
    "\n",
    "မြက်လဲစား၊ ခြေ ၄ ချောင်းနဲ့လဲ လမ်းလျှောက်ဖို့ Probability, \n",
    "\n",
    "$$P(\\text{eat grass} \\cap \\text{walk on 4}) = \\frac{2+4}{20} = \\frac{6}{20}$$\n",
    "\n",
    "ရှိတယ်။\n",
    "\n",
    "**အပေါ်က Probability ၃ ခုလုံး ပိုင်းခြေ (demominator) က တူတူပဲ 20 ပဲ။**\n",
    "\n",
    "မြက်စားတဲ့ ကောင်တွေထဲမှာ ခြေ ၄ ချောင်းနဲ့ လမ်းလျှောက်ဖို့ Probability ကျတော့ ပိုင်းခြေ ပြောင်းသွားတယ်။ \n",
    "\n",
    "$$P(\\text{walk on 4 given eat grass}) = P(\\text{walk on 4} | \\text{eat grass}) = \\frac{2 + 4}{2 + 4 + 5} = \\frac{6}{11}$$\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$$P(\\text{eat grass given walk on 4}) = P(\\text{eat grass} | \\text{walk on 4}) = \\frac{2 + 4}{2 + 2 + 2 + 4} = \\frac{6}{10}$$\n",
    "\n",
    "အပေါ်က ၂ ခုကို Conditional Probability လို့ ခေါ်တယ်။ စာဆန်ဆန် ရေးရရင် ... \n",
    "\n",
    "$$P(\\text{walk on 4} | \\text{eat grass}) = \\frac{P(\\text{walk on 4} \\cap \\text{eat grass})}{P(\\text{eat grass})}$$\n",
    "\n",
    "လက်တွေ့မှာ တခုချင်းစီကို သိနိုင်ပေမဲ့ $P(\\text{walk on 4} \\cap \\text{eat grass})$ ကို သိဖို့ မလွယ်ဘူး။ လက်တွေ့ ပြဿနာက ကိုယ် သိထားတဲ့ တိရစ္ဆာန်ဥယျာဉ်မှာ မြက်စားတဲ့ Probability $P(\\text{eat grass}) = 11/20 = 0.55$ ၊ မြက်စားပြီး ၄ ချောင်းနဲ့ လျှောက်တဲ့ Probability $P(\\text{walk on 4} | \\text{eat grass}) = 0.5454$ နဲ့ ၄ ချောင်းလျှောက်မဲ့ Probability $P(\\text{walk on 4}) = 0.5$ ရှိပေမဲ့ ကိုယ်မသိတဲ့ တိရစ္ဆာန်ဥယျာဥ်က အကောင်တကောင် လွတ်လာတဲ့အခါ ခြေလေးချောင်း လျှောက်/မလျှောက်ပဲ မြင်ရတယ်။ အထဲမှာ ဘယ်နှကောင် မြက်စားသလဲ၊ စုစုပေါင်း ဘယ်နှကောင် ရှိသလဲ မသိဘူး။ ကိုယ် အသက်အန္တရာယ် ရှိ/မရှိက ... $P(\\text{eat grass} | \\text{walk on 4})$ ပေါ်မှာ လုံးဝ မူတည်နေပြီ။\n",
    "\n",
    "$$P(\\text{eat grass} | \\text{walk on 4}) = \\frac{P(\\text{eat grass} \\cap \\text{walk on 4})}{P(\\text{walk on 4})} \\implies P(\\text{eat grass} | \\text{walk on 4}) P(\\text{walk on 4}) = P(\\text{eat grass} \\cap \\text{walk on 4})$$\n",
    "\n",
    "$$P(\\text{walk on 4} | \\text{eat grass}) = \\frac{P(\\text{walk on 4} \\cap \\text{eat grass})}{P(\\text{eat grass})} \\implies P(\\text{walk on 4} | \\text{eat grass}) P(\\text{eat grass}) = P(\\text{eat grass} \\cap \\text{walk on 4})$$\n",
    "\n",
    "$$\\therefore P(\\text{eat grass} | \\text{walk on 4}) P(\\text{walk on 4}) = P(\\text{walk on 4} | \\text{eat grass}) P(\\text{eat grass})$$\n",
    "\n",
    "$$\\implies P(\\text{eat grass} | \\text{walk on 4}) = \\frac{P(\\text{walk on 4} | \\text{eat grass}) P(\\text{eat grass})}{P(\\text{walk on 4})}$$\n",
    "\n",
    "This is Bayes Theorem. It also works on **continuous probabilities.**\n",
    "\n",
    "* `MultinomialNB` works on count (or tf-idf) feature `X`\n",
    "* `GaussianNB` works on continuous value feature `X`\n",
    "* `ComplementNB` works on imbalance datasets (one class is so less presented)\n",
    "* `BurnoulliNB` works better on shorter documents compared to `MultinomialNB`\n",
    "* `CategoricalNB` works on ordinal value feature `X` (each column must have 0, ..., n-1 values exactly where n is number of possible value for the column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = ds.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "selector = ds_y > ds_y.median()\n",
    "ds_y[selector] = 1\n",
    "ds_y[~selector] = 0\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y)\n",
    "model = nb.GaussianNB()\n",
    "model.fit(tr_X.values[:, :4], tr_y)\n",
    "pred = model.predict(ts_X.values[:, :4])\n",
    "print (metrics.classification_report(ts_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = ds.load_wine(return_X_y=True, as_frame=True)\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y)\n",
    "model = nb.ComplementNB()\n",
    "model.fit(tr_X.values[:, :4], tr_y)\n",
    "pred = model.predict(ts_X.values[:, :4])\n",
    "print (metrics.classification_report(ts_y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Few Words on Burmese Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyidaungsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['မင်း', 'ဒီ', 'မှာ', 'ဘာ', 'လုပ်', 'နေ', 'တာ', 'လဲ', '?']\n",
      "['မင်္ဂလာ', 'ပါ', '။', 'Data', 'Science', 'Using', 'PythonWeek16', 'မှ', 'ကြိုဆို', 'ပါ', 'တယ်', '။', 'ဒီ', 'နေ့', 'လေ့လာ', 'ဆွေးနွေးမဲ့', 'အကြောင်း', 'အရာ', 'က', 'Bayesian', 'Learners', 'တွေ', 'ပဲ', 'ဖြစ်', 'ပါ', 'တယ်', '။']\n"
     ]
    }
   ],
   "source": [
    "import pyidaungsu as pds\n",
    "\n",
    "print (pds.tokenize('မင်းဒီမှာဘာလုပ်နေတာလဲ?'))\n",
    "print (pds.tokenize('မင်္ဂလာပါ။ Data Science Using Python Week 16 မှ ကြိုဆိုပါတယ်။ ဒီနေ့ လေ့လာဆွေးနွေးမဲ့ အကြောင်းအရာက Bayesian Learners တွေပဲ ဖြစ်ပါတယ်။', form=\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['?' 'bayesianlearners' 'data' 'scienceusingpythonweek16' 'က' 'ကြိုဆို'\n",
      " 'ဆွေးနွေးမဲ့' 'တယ်' 'တာ' 'တွေ' 'ဒီ' 'နေ' 'နေ့' 'ပါ' 'ပဲ' 'ဖြစ်' 'ဘာ'\n",
      " 'မင်း' 'မင်္ဂလာ' 'မှ' 'မှာ' 'လုပ်' 'လေ့လာ' 'လဲ' 'အကြောင်း' 'အရာ' '။']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/anaconda3/envs/py37-dsup/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['e', 'f', 'fore', 'forehand', 'leven', 'm', 'reafter', 'reby', 'theless', 'ther', 'toge', 'whe'] not in stop_words.\n",
      "  % sorted(inconsistent)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>bayesianlearners</th>\n",
       "      <th>data</th>\n",
       "      <th>scienceusingpythonweek16</th>\n",
       "      <th>က</th>\n",
       "      <th>ကြိုဆို</th>\n",
       "      <th>ဆွေးနွေးမဲ့</th>\n",
       "      <th>တယ်</th>\n",
       "      <th>တာ</th>\n",
       "      <th>တွေ</th>\n",
       "      <th>...</th>\n",
       "      <th>မင်း</th>\n",
       "      <th>မင်္ဂလာ</th>\n",
       "      <th>မှ</th>\n",
       "      <th>မှာ</th>\n",
       "      <th>လုပ်</th>\n",
       "      <th>လေ့လာ</th>\n",
       "      <th>လဲ</th>\n",
       "      <th>အကြောင်း</th>\n",
       "      <th>အရာ</th>\n",
       "      <th>။</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313026</td>\n",
       "      <td>0.313026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313026</td>\n",
       "      <td>0.313026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.216731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.284975</td>\n",
       "      <td>0.216731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ?  bayesianlearners      data  scienceusingpythonweek16         က  \\\n",
       "0  0.341426          0.000000  0.000000                  0.000000  0.000000   \n",
       "1  0.000000          0.000000  0.313026                  0.313026  0.000000   \n",
       "2  0.000000          0.284975  0.000000                  0.000000  0.284975   \n",
       "\n",
       "    ကြိုဆို  ဆွေးနွေးမဲ့       တယ်        တာ       တွေ  ...      မင်း  \\\n",
       "0  0.000000     0.000000  0.000000  0.341426  0.000000  ...  0.341426   \n",
       "1  0.313026     0.000000  0.238065  0.000000  0.000000  ...  0.000000   \n",
       "2  0.000000     0.284975  0.216731  0.000000  0.284975  ...  0.000000   \n",
       "\n",
       "    မင်္ဂလာ        မှ       မှာ      လုပ်     လေ့လာ        လဲ  အကြောင်း  \\\n",
       "0  0.000000  0.000000  0.341426  0.341426  0.000000  0.341426  0.000000   \n",
       "1  0.313026  0.313026  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.284975  0.000000  0.284975   \n",
       "\n",
       "        အရာ         ။  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.476129  \n",
       "2  0.284975  0.216731  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text as text_fe\n",
    "from functools import partial\n",
    "word_tokenizer = partial(pds.tokenize, form=\"word\")\n",
    "tfidf_transformer = text_fe.TfidfVectorizer(input=\"content\", use_idf=True, norm=\"l2\", stop_words=\"english\", tokenizer=word_tokenizer)\n",
    "burmese_data = np.array([\n",
    "    \"မင်းဒီမှာဘာလုပ်နေတာလဲ?\",\n",
    "    \"မင်္ဂလာပါ။ Data Science Using Python Week 16 မှ ကြိုဆိုပါတယ်။\",\n",
    "    \"ဒီနေ့ လေ့လာဆွေးနွေးမဲ့ အကြောင်းအရာက Bayesian Learners တွေပဲ ဖြစ်ပါတယ်။\"\n",
    "])\n",
    "burmese_feaures = tfidf_transformer.fit_transform(burmese_data)\n",
    "feature_words = tfidf_transformer.get_feature_names_out()\n",
    "print (feature_words)\n",
    "pd.DataFrame(burmese_feaures.todense(), columns=feature_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9c1e998c7d6d5f29587b2c70e9bd488bb486b902354401efc27ca8457f04e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
