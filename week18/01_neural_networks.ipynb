{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "မင်္ဂလာပါ၊ ဒီနေ့ ဆွေးနွေးမဲ့ ခေါင်းစဉ်ကတော့ Neural Network ပဲ ဖြစ်ပါတယ်။ \n",
    "\n",
    "## Biological Parallel\n",
    "\n",
    "အဆင့်မြင့် သက်ရှိတွေရဲ့ ဦးဏှောက်တွေထဲမှာ Neurone တွေ ရှိကြောင်း သိခဲ့တာ နှစ်တရာနီးပါး ရှိပါပြီ။\n",
    "\n",
    "![](https://teachmeanatomy.info/wp-content/uploads/Structure-of-a-Neurone.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အပေါ်က ပုံမှာ ပြထားတဲ့အတိုင်း ဦးဏှောက်ထဲက Neurone တခုမှာ \n",
    "\n",
    "* Dendrite\n",
    "* Cell body (Nucleus ပါတဲ့ အပိုင်း)\n",
    "* Axon နဲ့ \n",
    "* Axon Terminal ဆိုပြီး အပိုင်းတွေ ပါဝင်ပါတယ်။ \n",
    "\n",
    "လူ့ဦးဏှောက်ထဲမှာ အဲဒီလိုမျိုး Neurone တွေ ဘီလျံနဲ့ချီပြီး ပါဝင်ကြပါတယ်။ \n",
    "\n",
    "Dendrite တွေက Input function ကို ထမ်းဆောင်ပါတယ်။ \n",
    "\n",
    "Cell body ကတော့ Dendrite က ရတဲ့ Input ကို Axon ကတဆင့် နောက်ထပ် Neurone တွေကို လက်ဆင့်ကမ်းမလားလို့ ဆုံးဖြတ်ပါတယ်။ \n",
    "\n",
    "Axon တွေကတဆင့် Neurone Firing ကို နောက် Neurone တခုရဲ့ Dendrite တွေကို လက်ဆင့်ကမ်းပါတယ်။ \n",
    "\n",
    "## Application in Machine Learning\n",
    "\n",
    "အစောပိုင်း သီအိုရီတွေအရတော့ Neurone တွေ စုပေါင်းပြီး သက်ရှိတွေဟာ Learning လုပ်ကြတယ်လို့ ယူဆကြပါတယ်။\n",
    "\n",
    "> ခုနောက်ပိုင်းတော့ အဲဒီလိုလဲ မဟုတ်သေးဘူးတဲ့\n",
    "\n",
    "ဒီတော့ Neurone တွေ အလွှာလိုက် အလွှာလိုက်မှာ ရှိနေတဲ့ state ဟာ သက်ရှိတွေရဲ့ မှတ်ဉာဏ် (memory) သဘောလိုမျိုးပဲလို့ ယူဆကြပါတယ်။ \n",
    "\n",
    "သင်္ချာနည်းအရလဲ ဒီလို တည်ဆောက်ထားတဲ့ Neural Network တခုဟာ Random Forest တွေလိုပဲ Learning capability ရှိတယ်လို့ သက်သေပြနိုင်ပါတယ်။ \n",
    "\n",
    "Machine Learning မှာ သုံးတဲ့ Neural Network ကို Neurone ကို အခြေခံထားတဲ့ Perceptron တွေနဲ့ တည်ဆောက်ထားပါတယ်။ \n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*n6sJ4yZQzwKL9wnF5wnVNg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အပေါ်က ပုံမှာ Perceptron တခုကို ပြထားပါတယ်။ \n",
    "\n",
    "* Dendrite တွေလိုပဲ Perceptron တခုမှာ input $x_1$ ကနေ $x_n$ အထိ ရှိပါတယ်။ \n",
    "  > ဒီနေရာက $n$ ဟာ အစောပိုင်း သင်ခန်းစာတွေမှာပါတဲ့ sample အရေအတွက် (row အရေအတွက်) $n$ နဲ့ မတူတာကို သတိပြုပါ။\n",
    "\n",
    "* Cell body + Axon လိုပဲ input signal တွေကို အခြေခံပြီး output signal ထုတ်ပေး မပေးကို $\\Sigma$ နဲ့ step function $s$ တို့က ဆုံးဖြတ်ပါတယ်။\n",
    "\n",
    "* ထွက်လာတဲ့ output signal ကို နောက်ထပ် **layer** တခုက input အဖြစ် အသုံးပြုပါတယ်။\n",
    "\n",
    "**ဒီ perceptron ဟာ အရင်က သိခဲ့ပြီးသား ဘယ် model နဲ့ တူပါသလဲ ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သင်္ချာ formula အနေနဲ့ Perceptron တလုံးတည်းဟာ အောက်ပါ function ကို တွက်ချက်ပေးပါတယ်။ \n",
    "\n",
    "$$y = W^TX + w_0$$\n",
    "\n",
    "where $y$ is output, $X$ is input vector $\\begin{bmatrix}x_1 & x_2 & \\dots & x_n\\end{bmatrix}^T$ and $W=\\begin{bmatrix}w_1 & w_2 & \\dots & w_n\\end{bmatrix}$ and $w_0$ are model parameters.\n",
    "\n",
    "Model က တွက်ထုတ်ပေးတဲ့ function ကို define တာ $w_0, w_1, w_2, \\dots, w_n$ တို့ ပါဝင်တဲ့ weights တွေ ဖြစ်ပါတယ်။ \n",
    "\n",
    "အဲဒီ weight တွေထဲကမှ $w_0$ က model ရဲ့ bias term ဖြစ်ပြီး ကျန်တာတွေကတော့ bias မဟုတ်တဲ့ (data ပေါ် မူတည်တဲ့) term တွေ ဖြစ်ပါတယ်။\n",
    "\n",
    "* **ဘာလို့ $w_0$ ဟာ bias term ဖြစ်ရတာလဲ။**\n",
    "\n",
    "* **အခုရော perceptron နဲ့ တူတဲ့ model ကို သိပြီလား။**\n",
    "\n",
    "Step function အနေနဲ့ traditionally က Sigmoid function ကို သုံးခဲ့ပြီး ခုနောက်ပိုင်းမှာတော့ ReLU နဲ့ Leaky ReLU တို့ကို သုံးပါတယ်။ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Tali-Leibovich-Raveh/publication/325868989/figure/fig2/AS:639475206074368@1529474178211/A-Basic-sigmoid-function-with-two-parameters-c1-and-c2-as-commonly-used-for-subitizing.png)\n",
    "\n",
    "![](https://www.researchgate.net/profile/Hossam-H-Sultan/publication/333411007/figure/fig7/AS:766785846525952@1559827400204/ReLU-activation-function.png)\n",
    "\n",
    "ဒီ သဘောတရားတွေကို အသုံးပြုပြီး **Artificial Neural Network** တွေကို တည်ဆောက်ပါတယ်။ \n",
    "\n",
    "## Artificial Neural Network\n",
    "\n",
    "![](https://pimages.toolbox.com/wp-content/uploads/2022/05/18113202/The-Architecture-of-a-Neural-Network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီပုံမှာ layer 5 ခုပါတဲ့ artificial neural network တခုကို ပြထားပါတယ်။ \n",
    "\n",
    "* အလုံးလေး တလုံးစီက ကျနော်တို့ စောစောက ပြောခဲ့တဲ့ perceptron တခုစီ ဖြစ်ပါတယ်။ \n",
    "\n",
    "* ပထမဆုံး (ဘယ်ဘက်ဆုံး) layer ဟာ input layer ဖြစ်ပါတယ်။ \n",
    "\n",
    "* နောက်ဆုံး (ညာဘက်ဆုံး) layer ဟာ output layer ဖြစ်ပါတယ်။ \n",
    "\n",
    "* ကြားထဲက layer တွေကို hidden layer တွေလို့ ခေါ်ပါတယ်။\n",
    "  \n",
    "* ဒီနေ့ `sklearn.neural_network` မှာပါတဲ့ `MLPRegressor` နဲ့ `MLPClassifier` တို့ကိုပဲ focus လုပ်ပါမယ်။\n",
    "  * `sklearn` implementation မှာ fully connected network ကိုပဲ support လုပ်ပါတယ်။\n",
    "  * Neural Network တွေဟာ computationally **expensive** ဖြစ်ပါတယ်။ \n",
    "  * ဒါကြောင့် practice မှာ pytorch or tensorflow library တွေနဲ့ GPU ပေါ်မှာ တွက်ကြပါတယ်။ \n",
    "\n",
    "* **Exercise** -- calculate how many weights does the neural network in the figure have ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation Aspects\n",
    "\n",
    "ဒီလို weight အရေအတွက် အများအပြားကို optimize လုပ်ဖို့ရာ အရင်သိခဲ့တဲ့ Matrix နည်း၊ Differentiate နည်း (gradient descend) တွေနဲ့ အဆင်မပြေတော့ပါဘူး။ \n",
    "\n",
    "ဒါကြောင့် stochastic gradient descend ကို သုံးပါတယ်။ \n",
    "\n",
    "ဒါလဲ efficient မဖြစ်သေးပဲ ဖြစ်နေပါသေးတယ်။ \n",
    "\n",
    "ဒါကြောင့် gradient descend နဲ့ သီအိုရီအရ တူတဲ့ back propagation နည်းကို အသုံးပြုပါတယ်။ \n",
    "\n",
    "အသေးစိတ် တွက်ချက်တာတွေကတော့ ဒီ course ရဲ့ scope အပြင်ဘက်မှာမို့လို့ မပြောတော့ပါဘူး။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pyplot\n",
    "from sklearn import datasets as ds, metrics, model_selection as ms, neural_network as nn, preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778849647370551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neola\\anaconda3\\envs\\py37-dsup\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "df_X, ds_y = ds.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "X = ss.fit_transform(tr_X)\n",
    "\n",
    "nn_model = nn.MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10), \n",
    "    activation=\"relu\", \n",
    "    solver=\"adam\", \n",
    "    alpha=0.0001, \n",
    "    batch_size=100, \n",
    "    learning_rate_init=0.001, \n",
    "    max_iter=200, \n",
    "    random_state=42, \n",
    "    momentum=0.9,\n",
    "    verbose=False\n",
    ")\n",
    "nn_model.fit(X, tr_y)\n",
    "\n",
    "pred_y = nn_model.predict(ss.transform(ts_X))\n",
    "print (metrics.r2_score(ts_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 2) Processing standardscaler, total=   0.0s\n",
      "[Pipeline] ...... (step 2 of 2) Processing mlpregressor, total=  11.1s\n",
      "0.778849647370551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neola\\anaconda3\\envs\\py37-dsup\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "# the following is more stylish but equivalent way of writing above\n",
    "from sklearn import pipeline\n",
    "\n",
    "df_X, ds_y = ds.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "\n",
    "nn_model = nn.MLPRegressor(\n",
    "    hidden_layer_sizes=(10, 10), \n",
    "    activation=\"relu\", \n",
    "    solver=\"adam\", \n",
    "    alpha=0.0001, \n",
    "    batch_size=100, \n",
    "    learning_rate_init=0.001, \n",
    "    max_iter=200, \n",
    "    random_state=42, \n",
    "    momentum=0.9,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_pipeline = pipeline.make_pipeline(\n",
    "    ss, nn_model\n",
    ", verbose=True)\n",
    "\n",
    "model_pipeline.fit(tr_X, tr_y)\n",
    "pred_y = model_pipeline.predict(ts_X)\n",
    "\n",
    "print (metrics.r2_score(ts_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Things We Discussed Today\n",
    "\n",
    "1. Neural Networks\n",
    "2. `sklearn.preprocessing.StandardScalar` (earlier, we use `Normalize` with axis=1)\n",
    "3. `sklearn.pipeline.make_pipeline` (earlier, we code ourselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] .... (step 1 of 2) Processing standardscaler, total=   0.0s\n",
      "[Pipeline] ..... (step 2 of 2) Processing mlpclassifier, total=   0.4s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        54\n",
      "           1       0.99      0.99      0.99        89\n",
      "\n",
      "    accuracy                           0.99       143\n",
      "   macro avg       0.99      0.99      0.99       143\n",
      "weighted avg       0.99      0.99      0.99       143\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neola\\anaconda3\\envs\\py37-dsup\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "df_X, ds_y = ds.load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "ss = pp.StandardScaler()\n",
    "\n",
    "nn_model = nn.MLPClassifier(\n",
    "    hidden_layer_sizes=(10, 10), \n",
    "    activation=\"relu\", \n",
    "    solver=\"adam\", \n",
    "    alpha=0.0001, \n",
    "    batch_size=100, \n",
    "    learning_rate_init=0.001, \n",
    "    max_iter=200, \n",
    "    random_state=42, \n",
    "    momentum=0.9,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model_pipeline = pipeline.make_pipeline(\n",
    "    ss, nn_model\n",
    ", verbose=True)\n",
    "\n",
    "model_pipeline.fit(tr_X, tr_y)\n",
    "pred_y = model_pipeline.predict(ts_X)\n",
    "\n",
    "print (metrics.classification_report(ts_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = ds.load_digits(return_X_y=True, as_frame=True)\n",
    "tr_X, ts_X, tr_y, ts_y = ms.train_test_split(df_X, ds_y, random_state=42)\n",
    "\n",
    "# build nn model with different size and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# build nn model with different size and shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf5841eeb9fdd70b5a32cd740512a3aae54df2b4f66ae714b228eb262545450d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
