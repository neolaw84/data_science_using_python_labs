{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Part 2\n",
    "\n",
    "မင်္ဂလာပါ။ Data Science using Python - Week 9 - Day 2 - Pandas - Part 2 မှ ကြိုဆိုပါတယ်။ \n",
    "\n",
    "ဒီကနေ့ အစီအစဉ်ကတော့ ... \n",
    "\n",
    "* SQL via `pandas` \n",
    "* Data Types and Date/Time in `pandas`\n",
    "* Missing Data Handling\n",
    "* `groupby`\n",
    "* `DataFrame` manipulation \n",
    "\n",
    "တို့ပဲ ဖြစ်ပါတယ်။"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL via `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip\n",
    "!unzip chinook.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format of sqlite path are: \n",
    "# sqlite:// + /relative/path/from/current/directory.db\n",
    "# sqlite:/// + /absolute/path/to/data.db\n",
    "# sqlite:// + <nothing for in memory db>\n",
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "with engine.connect() as con:\n",
    "  df = pd.read_sql(\"SELECT * FROM INVOICES;\", con=con)\n",
    "  print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Crash Course to SQLite\n",
    "\n",
    "**SQL** or Structured Query Language ဆိုတာ C/C++ လိုပဲ ISO standard တခုဖြစ်ပြီး Relational Database တွေကို Query လုပ်ဖို့ တီထွင်ထားတဲ့ Language ဖြစ်တယ်။ Database internal ကလီစာတွေဖြစ်တဲ့ Cache size, loop, query optimization တွေ သိစရာမလိုပဲ ကိုယ်လိုချင်တာ ကိုယ်ပြောရတဲ့ Language မို့လို့ Declarative Language တမျိုးလဲ ဖြစ်တယ်။ Microsoft, IBM နဲ့ Oracle တို့လို commercial database vendor တွေတင်မကပဲ MySQL နဲ့ PostgreSQL တို့လိုမျိုး non-commercial database တွေကပါ လိုက်နာကြရတဲ့ စံနှုန်းဖြစ်တယ်။\n",
    "\n",
    "> တကယ်တမ်းမှာတော့ PostgreSQL ကလွဲလို့ ISO စံအတိုင်း လိုက်နာတဲ့ RDBMS က ရှားပါးပါတယ်။ \n",
    "\n",
    "> relational database မဟုတ်တဲ့ mongoDb, HBase နဲ့ Cassandra လိုဟာမျိုးတွေကို NoSQL Database လို့ ခေါ်ကြတယ်။\n",
    "\n",
    "SQLite ဆိုတာ Python နဲ့ အတူ built-in တွဲပါတဲ့ relational database တခုဖြစ်ပြီး testing အတွက် အသုံးကျတယ်။ \n",
    "\n",
    "**chinook** ဆိုတာက Microsoft Access နဲ့ အတူတွဲပါတဲ့ tradewind လိုပဲ နမူနာ စမ်းလို့ရအောင် လုပ်ထားတဲ့ Dataset တခု ဖြစ်တယ်။ သူ့ရဲ့ Entity Relationship Diagram ကို အောက်မှာ တွေ့နိုင်တယ်။ \n",
    "\n",
    "![](https://www.sqlitetutorial.net/wp-content/uploads/2015/11/sqlite-sample-database-color.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` ကနေ Relational Database တွေကို (`sqlalchemy` ကတဆင့်) access လုပ်ရတာ အင်မတန် လွယ်ပါတယ်။ စုစုပေါင်း program code 3 ကြောင်းပဲ လိုပါတယ်။\n",
    "\n",
    "```python\n",
    "# line 1 : create_engine\n",
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "# line 2 : connect to db\n",
    "with engine.connect() as con:\n",
    "  # line 3 : read_sql\n",
    "  df = pd.read_sql(\"SELECT * FROM INVOICES;\", con=con)\n",
    "  print(df.head())\n",
    "```\n",
    "\n",
    "1. ပထမဆုံး line 1 အနေနဲ့ `sqlalchemy` ရဲ့ `create_engine` ကို ခေါ်ရပါမယ်။ ဒီစာကြောင်းက `sqlalchemy` library ကို ကိုယ်ချိတ်မဲ့ database အကြောင်း အသိပေးလိုက်တာပဲ ဖြစ်ပြီး ဘာချိတ်ဆက်မှုမှ မလုပ်ရသေးပါဘူး။ password မှားတာ၊ firewall ကာထားတာ၊ whitelist/blacklist မရှင်းရသေးတာတွေကြောင့် ဖြစ်တဲ့ error တွေ ဒီတဆင့်မှာ **မပေါ်ပါဘူး**။ ဒါကို debug လုပ်တဲ့အခါ သတိထားပါ။ အကျွမ်းတဝင်မရှိတဲ့သူတွေက `create_engine` ok ရင် database နဲ့ ချိတ်လို့ ရနေပြီလို့ ထင်တတ်ပါတယ်။\n",
    "\n",
    "`create_engine` ရဲ့ url format က \n",
    "\n",
    "> `dialect+driver://username:password@host:port/database`\n",
    "\n",
    "ဖြစ်ပါတယ်။\n",
    "\n",
    "အခု ပြထားတာက postgresql ရဲ့ url နမူနာတွေဖြစ်ပါတယ်။\n",
    "\n",
    "```python\n",
    "# default\n",
    "engine = create_engine('postgresql://scott:tiger@localhost/mydatabase')\n",
    "\n",
    "# psycopg2\n",
    "engine = create_engine('postgresql+psycopg2://scott:tiger@localhost/mydatabase')\n",
    "\n",
    "# pg8000\n",
    "engine = create_engine('postgresql+pg8000://scott:tiger@localhost/mydatabase')\n",
    "```\n",
    "\n",
    "အခြား database url နမူနာတွေနဲ့ နောက်ဆုံး support လုပ်တဲ့ database အမျိုးအစားများကို [ဒီလင့်ခ်မှာ](https://docs.sqlalchemy.org/en/14/core/engines.html#supported-databases) ကြည့်နိုင်ပါတယ်။\n",
    "\n",
    "2. ဒုတိယ line 2 မှာ database ကို `connect` လုပ်ရမှာ ဖြစ်ပါတယ်။ ဒီ `connect` function က connection object return ပြန်ပေးပါတယ်။ အဲဒီ connection ဟာ `closeable` ဖြစ်ပြီး **မမေ့မလျော့** ပြန်ပိတ်ဖို့လိုပါတယ်။ အားလုံး သိကြတဲ့အတိုင်း ပြန်ပိတ်ဖို့ မမေ့အောင် `with` ဆိုပြီး context manager နဲ့ ဖြေရှင်းနိုင်ပါတယ်။\n",
    "\n",
    "3. တတိယ line 3 မှာ `pd.read_sql` ဆိုပြီး data ကို read ရမှာ ဖြစ်ပါတယ်။ ဒီနေရာမှာ ပထမ positional argument က sql statement ဖြစ်ပါတယ်။ ပုံမှန်အားဖြင့် ဒီလို ရေးရပါတယ်။\n",
    "\n",
    "```sql\n",
    "SELECT col_1 AS alias_1, col_2 AS alias_2, ...\n",
    "FROM database_object_name\n",
    "WHERE predicate_1 AND predicate_2\n",
    "OR predicate_3 AND predicate_4;\n",
    "```\n",
    "\n",
    "database_object_name ဟာ table သို့မဟုတ် view ဖြစ်နိုင်ပါတယ်။\n",
    "predicate ဆိုတာ boolean condition မျိုးတွေပါပဲ။\n",
    "\n",
    "ဒီ course က SQL သင်တန်းမဟုတ်လို့ အကျယ်မပြောတော့ပါ။\n",
    "\n",
    "ဒုတိယ `con` ဆိုတဲ့ argument မှာ ဖွင့်ထားတဲ့ connection object ကို ပေးရပါမယ်။"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame in *ad hoc* manner\n",
    "\n",
    "`pandas` မှာ DataFrame တခုကို ဒီလိုဖန်တီးနိုင်ပါတယ်။ \n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(data={\"col_a\" : [1, 2, 3], \"col_b\" : [\"a\", \"b\", \"c\"]})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test create one\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "နောက်ထပ်ပုံစံတမျိုးအနေနဲ့ `numpy` array သို့မဟုတ် `list` of `list` နဲ့လဲ ဖန်တီးနိုင်ပါတယ်။\n",
    "\n",
    "```python\n",
    "df = pd.DataFrame(data=np.array([[1, \"a\"], [2, \"b\"], [3, \"c\"]]), columns=[\"col_a\", \"col_b\"])\n",
    "# or \n",
    "df = pd.DataFrame(data=[[1, \"a\"], [2, \"b\"], [3, \"c\"]], columns=[\"col_a\", \"col_b\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database သို့ Data ပြန်ရေးခြင်း**\n",
    "\n",
    "ဖတ်တုန်းကလိုပဲ ရေးဖို့အတွက် 3 line of code ပဲ လိုပါတယ်။ \n",
    "\n",
    "```python\n",
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "with engine.connect() as con:\n",
    "    df.to_sql(\"my_table\", con=con, index=False, if_exists=\"fail\")\n",
    "```\n",
    "\n",
    "ကွာခြားသွားတာက တတိယ line 3 ပါပဲ။ ပထမဆုံး positional argument က table_name ကို ပေးရပါတယ်။ `con` argument က တူတူပဲ ဖြစ်ပြီး `index` ဆိုတာကတော့ index တွေကိုပါ ထည့်မရေးစေချင်ရင် `False` လို့ပေးရပါမယ်။ နောက်ဆုံး `if_exists` ကတော့ `\"fail\"`, `\"replace\"` နဲ့ `\"append\"` သုံးမျိုး ဖြစ်နိုင်ပါတယ်။ `dtype` ဆိုတဲ့ အထဲမှာတော့ column name တွေက key, `sqlalchemy.types` အောက်က အောက်ပါ object တွေက value အဖြစ် ပါတဲ့ `dict` တခု ပေးရပါမယ်။ \n",
    "\n",
    "**BigInteger**\n",
    "\n",
    "A type for bigger int integers.\n",
    "\n",
    "**Boolean**\n",
    "\n",
    "A bool datatype.\n",
    "\n",
    "**Date**\n",
    "\n",
    "A type for datetime.date() objects.\n",
    "\n",
    "**DateTime**\n",
    "\n",
    "A type for datetime.datetime() objects.\n",
    "\n",
    "**Enum**\n",
    "\n",
    "Generic Enum Type.\n",
    "\n",
    "**Float**\n",
    "\n",
    "Type representing floating point types, such as FLOAT or REAL.\n",
    "\n",
    "**Integer**\n",
    "\n",
    "A type for int integers.\n",
    "\n",
    "**Interval**\n",
    "\n",
    "A type for datetime.timedelta() objects.\n",
    "\n",
    "**LargeBinary**\n",
    "\n",
    "A type for large binary byte data.\n",
    "\n",
    "**MatchType**\n",
    "\n",
    "Refers to the return type of the MATCH operator.\n",
    "\n",
    "**Numeric**\n",
    "\n",
    "Base for non-integer numeric types, such as NUMERIC, FLOAT, DECIMAL, and other variants.\n",
    "\n",
    "**PickleType**\n",
    "\n",
    "Holds Python objects, which are serialized using pickle.\n",
    "\n",
    "**SchemaType**\n",
    "\n",
    "Mark a type as possibly requiring schema-level DDL for usage.\n",
    "\n",
    "**SmallInteger**\n",
    "\n",
    "A type for smaller int integers.\n",
    "\n",
    "**String**\n",
    "\n",
    "The base for all string and character types.\n",
    "\n",
    "**Text**\n",
    "\n",
    "A variably sized string type.\n",
    "\n",
    "**Time**\n",
    "\n",
    "A type for datetime.time() objects.\n",
    "\n",
    "**Unicode**\n",
    "\n",
    "A variable length Unicode string type.\n",
    "\n",
    "**UnicodeText**\n",
    "\n",
    "An unbounded-length Unicode string type.\n",
    "\n",
    "> be careful! sqlalchemy's type inference is **very very BAD**. It's a good practice *to always provide `dtype`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "with engine.connect() as con:\n",
    "    df.to_sql(\"my_table\", con=con, index=False, if_exists=\"fail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it again. it will error out since the table exists. \n",
    "# change if_exists to make it not error out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pandas` Data Types\n",
    "\n",
    "အရင်နေ့က `dtypes` ဆိုတဲ့ property ကို သုံးပြီး data type တွေကို ကြည့်နိုင်တာ စမ်းကြည့်ပြီးပြီ။ ဒီနေ့ `pandas` Data type တွေကို အသေးစိတ် လေ့လာကြမယ်။\n",
    "\n",
    "```\n",
    "Pandas dtype\tPython type     Usage\n",
    "object\t        str or mixed\tText or mixed numeric and non-numeric values\n",
    "int<x>          int             Integer numbers \n",
    "                                \n",
    "                                \n",
    "float<x>        float\t        Floating point numbers\n",
    "                                \n",
    "bool\t        bool\t        True/False values\n",
    "datetime64      datetime lib    Date and time values\n",
    "timedelta[ns]\tdatetime lib\tDifferences between two datetimes\n",
    "category        NA              Finite list of text values\n",
    "```\n",
    "\n",
    "> int\\<x\\> means int8, int16, int32, int64 etc.\n",
    "\n",
    "> so is float\\<x\\>\n",
    "\n",
    "### `astype` function\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\"my_column\": [\"1\", \"2\", \"3\"]})\n",
    "print (df.my_column)\n",
    "my_column_as_int = df.my_column.astype(np.int)\n",
    "print (my_column_as_int.dtype)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try it; convert other data types as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one need to define sub-types of CategoricalDType (well, I use \"sub-type\" loosely here)\n",
    "cat_type = pd.api.types.CategoricalDtype(categories=[\"possible\", \"values\", \"here\"], ordered=True)\n",
    "df[\"cat_column\"] = [\"possible\", \"here\", \"here\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will see the data type as object here\n",
    "df.cat_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cat_column\"] = df.cat_column.astype(cat_type)\n",
    "df.cat_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will see an error here. why ?\n",
    "df[\"another_cat_column\"] = [\"value\", \"here\", \"don't\", \"belong\"]\n",
    "df[\"another_cat_column\"] = df.another_cat_column.astype(cat_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### date and time\n",
    "\n",
    "Problem အပေးဆုံး data type တွေကတော့ datetime ပါပဲ။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"period_column\"] = pd.period_range(start='2022-01-01', periods=3, freq='3 M') # default frequency is 'D' or '1 D'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will see very strange dtype 'period[3M]'\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting from period to datetime requires\n",
    "# 1. conversion to str and\n",
    "# 2. using pandas' to_datetime function and\n",
    "# 3. identification of format string (optional)\n",
    "df[\"dates_1\"] = pd.to_datetime(df.period_column.astype(str), format='%Y-%m')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "df[\"dates_2\"] = dt.now().strftime(\"%Y-%m-%d\")\n",
    "df[\"dates_2\"] = pd.to_datetime(df.dates_2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtracting two datetime64 will give you timedelta64\n",
    "df[\"dates_2\"] - df[\"dates_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why so many multiplication of 1k ?\n",
    "df[\"dates_1\"] + np.timedelta64(24*3600 * 1000 * 1000 * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's move on to Missing Data Handling\n",
    "\n",
    "missing data ဆိုတာ real-life မှာ ခဏခဏ တွေ့ရတယ်။ experiment တွေမှာ sample ပျက်သွားတာ၊ ပျောက်သွားတာ ဖြစ်တတ်တယ်။ web scraping လုပ်နေတုန်းမှာ internet ကျကွဲသွားတာတို့၊ တခြား error တချို့တို့ကြောင့် missing data ဖြစ်သွားတတ်သလို ... ဘယ်လိုပင် ကြိုးစား ထိန်းသိမ်းသော်လည်း အကြောင်းကြောင်းကြောင့် data ပျောက်နေတာတွေ ရှိတတ်တယ်။\n",
    "\n",
    "missing data ကို ဘာလုပ်ရမလဲဆိုတာ one-size-fits-all အဖြေ မရှိဘူး။ data ပေါ်မှာ မူတည်တယ်။ \n",
    "\n",
    "* score data/exam marks data\n",
    "* weather data (temperature/pressure etc.)\n",
    "* gender data or survey data asking what people likes from a list of 3 dishes\n",
    "\n",
    "`pandas` မှာ missing data/value တွေကို ကိုင်တွယ်ဖို့ \n",
    "\n",
    "* `dropna`\n",
    "* `fillna` နဲ့\n",
    "* `interpolate` ဆိုတဲ့ function ၃ ခုရှိတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md = pd.DataFrame({\n",
    "    \"score\" : [10, 3, np.nan, 7, 9],\n",
    "    \"temp\" : [42.0, 35.0, 22.8, np.nan, 30.8],\n",
    "    \"gender\": [\"M\", \"F\", np.nan, \"M\", \"F\"], \n",
    "})\n",
    "df_with_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md.dropna(axis=0, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md.dropna(axis=1) # oh O!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md.fillna({\"gender\": \"not_available\", \"score\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md.temp.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md.temp.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_md[\"temp\"].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting columns\n",
    "df_with_md.drop(\"temp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting rows\n",
    "df_with_md.drop([1, 3], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to handle **BIG Data** ?\n",
    "\n",
    "Please do NOT give the following answers in your interviews:\n",
    "\n",
    "* I don't know. That's what Data Engineers are supposed to do.\n",
    "* Buy me larger machines with 128GB of RAM.\n",
    "* I leave it run overnight and pick up the results in the morning.\n",
    "\n",
    "Instead, (depending on the interviewer) give these answers (or all of them):\n",
    "\n",
    "* I'll try to reduce the Big O notation of the operation if possible\n",
    "  * Equal-joins tend to be easier to reduce using hashes\n",
    "* I'll sample the data based on its distribution\n",
    "  * Central Limit Theorem etc.\n",
    "* I'll divide and conquer using `pandas` `DataFrame` `groupby`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "with engine.connect() as con:\n",
    "  df = pd.read_sql(\"SELECT * FROM INVOICES;\", con=con)\n",
    "  print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby(\"BillingCity\")\n",
    "for city, df_by_city in groups:\n",
    "    print (\"city : {}\".format(city))\n",
    "    print (df_by_city.shape)\n",
    "    print (df_by_city.head(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get max of all column by BillingCity\n",
    "groups.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups.apply(lambda x : x.select_dtypes(include=np.number).max() - x.select_dtypes(include=np.number).min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stuttgart = groups.get_group(\"Stuttgart\")\n",
    "df_stuttgart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stuttgart.loc[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this result with above's\n",
    "df_stuttgart = df_stuttgart.set_index(\"InvoiceId\", drop=True, inplace=False)\n",
    "df_stuttgart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stuttgart.loc[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stuttgart.reset_index(drop=False, inplace=True)\n",
    "df_stuttgart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly, the Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sa.create_engine(\"sqlite:///chinook.db\")\n",
    "with engine.connect() as con:\n",
    "  df_invoice_items = pd.read_sql(\"SELECT * FROM INVOICE_ITEMS;\", con=con)\n",
    "  print (df_invoice_items.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_items_with_invoice_data = pd.merge(left=df_invoice_items, right=df, on=\"InvoiceId\", how=\"left\")\n",
    "df_invoice_items_with_invoice_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2458648b5873a8511ce5d0b68ae5943af1ad0cfd25276a02b44cf6fbc34eca5c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
