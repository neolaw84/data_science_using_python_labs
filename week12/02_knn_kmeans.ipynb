{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Last Day\n",
    "\n",
    "ဒီနေ့ဟာ Data Science Using Python - Part 1 ရဲ့ နောက်ဆုံးနေ့ week 12 - day 2 ဖြစ်ပါတယ်။ \n",
    "\n",
    "## Review\n",
    "\n",
    "သိပြီးသမျှကို Review အနေနဲ့ ပြန်ပြောရရင် [Recipe အခု 20 ကို ဒီမှာ ရယူနိုင်ပါတယ်။](https://drive.google.com/file/d/19PU3XrrIDCMKy4eFaZJTZpEA9rCmQGuj/view?usp=sharing)\n",
    "\n",
    "သင်တန်းသားတွေ အနေနဲ့ အောက်ပါတို့ကို (Google ရှာပြီးဖြစ်ဖြစ် stack overflow မှာ ရှာပြီးဖြစ်ဖြစ် copy/paste လုပ်တာပဲဖြစ်ဖြစ်) လုပ်တတ်သင့်ပါပြီ။ \n",
    "\n",
    "ကိုယ့်ဟာကိုယ် မညာတမ်း ပြန်ဖြေကြည့်ပါ။ မရသေးရင် စမ်းပါ။\n",
    "\n",
    "1. Data တွေကို manipulate လုပ်တာ၊ slice/dice/selection etc. \n",
    "2. Linear equations/non-linear equations တွေကို ရှင်းတာ၊ minimise လုပ်တာ\n",
    "\n",
    "Assignment 1 (week 8) က ဒီ ၂ ခုကို check တာဖြစ်တယ်။\n",
    "\n",
    "3. သင်္ချာ function တခုကို python နဲ့ရေးတာ။ python code အစိမ်းတွေ့ရင် ဖတ်တတ်တာ။ \n",
    "4. Regression/classification/clustering တွေ ဘယ်လိုကွဲသလဲ ရှင်းပြနိုင်တာ။\n",
    "\n",
    "5. Linear regression/logistic regression/kmeans clustering model တွေဆောက်တာ၊ အဲဒီ model တွေရဲ့ performance ကိုတိုင်းတာ။ အဲဒီ‌ model တွေရဲ့ အရေးကြီးတဲ့ hyperparameter တွေကိုပြောင်းတာ၊ ပြန် train တာ။\n",
    "\n",
    "Assignment 2 (week 10) က ဒါကို တပိုင်းတစ စစ်ဆေးတာ ဖြစ်တယ်။\n",
    "\n",
    "Part 1 ရဲ့အဆုံး project က ဒီ ၅ ပိုင်းလုံး ကျွမ်းကျင်မှ လုပ်နိုင်လိမ့်မယ်။ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distance-based methods\n",
    "\n",
    "### What is the distance between two points ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import neighbors\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "X1 = 0\n",
    "X2 = 1\n",
    "data = np.array([\n",
    "    [3.0, 5.0],\n",
    "    [10.0, 1.0]\n",
    "])\n",
    "plt.scatter(data[:,X], data[:,X2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance between two vectors (points) $V_1 = \\langle v_{1,1}, v_{1,2} \\rangle$ and $V_2 = \\langle v_{2,1}, v_{2,2} \\rangle$ is given by: \n",
    "\n",
    "$$d(V_1, V_2) = \\sqrt{(v_{2,1} - v_{1,1})^2 + (v_{2,2} - v_{1,2})^2}$$\n",
    "\n",
    "This is known as **Euclidean Distance**. \n",
    "\n",
    "> $L_2$-norm is defined as the distance of a vector from origin $O$.\n",
    "\n",
    "> There are other distances known as **Manhattan Distance** and **Cosine Distance**. \n",
    "\n",
    "### Generalization of **Euclidean Distance**\n",
    "\n",
    "Generalization of Euclidean Distance is given as : \n",
    "\n",
    "$$d(V_1, V_2) = \\sqrt{(v_{2,1} - v_{1,1})^2 + (v_{2,2} - v_{1,2})^2 + ... + (v_{2,m} - v_{1,m})^2}$$\n",
    "\n",
    "$$=\\sqrt{\\sum_{k=1}^{m}{(v_{2,k} - v_{1,k})^2}}$$\n",
    "\n",
    "ဟိုတပါတ်က ဆွေးနွေးခဲ့တဲ့ `KMeans` က Euclidean Distance ကို သုံးတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "plt.scatter(df_X.Longitude, df_X.Latitude, marker=\".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cluster.KMeans()\n",
    "model.fit(df_X[[\"Longitude\", \"Latitude\"]])\n",
    "results = model.predict(df_X[[\"Longitude\", \"Latitude\"]])\n",
    "plt.scatter(df_X[\"Longitude\"], df_X[\"Latitude\"], s=1, c=results, cmap=\"plasma\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `KNeighborsClassifier` and `KNeighborsRegressor`\n",
    "\n",
    "နောက်ထပ် ဆွေးနွေးမဲ့ distance-based method နောက်တခုကတော့ `KNeighborsClassifier` နဲ့ `KNeighborsRegressor` ပဲ ဖြစ်တယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = neighbors.KNeighborsRegressor()\n",
    "tr_X, ts_X, tr_y, ts_y = model_selection.train_test_split(df_X, ds_y)\n",
    "model.fit(tr_X, tr_y)\n",
    "y_pred = model.predict(ts_X)\n",
    "print (metrics.r2_score(y_true=ts_y, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အရေးပါတဲ့ model parameter တွေက ... \n",
    "\n",
    "* `n_neighbors`\n",
    "* `weight` : either \"uniform\" or \"distance\"\n",
    "* `metric` : \"minkowski\" သို့မဟုတ် function တခု\n",
    "* `p` : 1 ဆိုရင် Manhattan Distance, 2 ဆိုရင် Euclidean Distance\n",
    "* `algorithm` : \"ball_tree\" or \"kd_tree\"\n",
    "\n",
    "KNN ကို dimension (columns/features) 15 ခု အောက်ဆိုရင် သုံးနိုင်တယ်လို့ အကြမ်းမှတ်လို့ ရတယ်။ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Project\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "[MNIST dataset](http://yann.lecun.com/exdb/mnist/) အပေါ်မှာ အောက်ပါ model တွေကို ဆောက်ပြီး ရလာတဲ့ Parameter/Classification Report တွေကို တင်ပြရမှာဖြစ်တယ်။ \n",
    "\n",
    "1. `LogisticRegression` model set 1\n",
    "\n",
    "* Use center/middle 18x18 pixels and only 30,000 samples. All parameters being equal with `penalty` = \"elasticnet\". Set `l1_ratio` to [0.0, 0.2, 0.5, 0.8, 1.0].\n",
    "\n",
    "2. `LogisticRegression` model set 2\n",
    "\n",
    "* Use center/middle 18x18 pixels and only 30,000 samples. All parameters being equal with 5 different values for C. Set `penalty` to the one that gives best `f1_score` from item 1.\n",
    "\n",
    "![](https://i.ibb.co/kDyVnF7/part-1-project-drawio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Augmenting with more **feature engineering** model set 3\n",
    "\n",
    "* Take all the **60,000** images. Put that into `scipy.linalg.svd` and get 100 most important features. \n",
    "* Using these 100 features, train a new set of `LogisticRegression` model with 5 different values for C. Use \"elasticnet\" and set `l1_ratio` to [0.5]\n",
    "    * Is this new model's Classification Report better than those from 1 ?\n",
    "    * Is this new model's Classification Report better than those from 2 ? \n",
    "\n",
    "4. Use `coef_` properties of the best model from 1 and 2 to determine the best 100 column (features) to be used - model set 4\n",
    "\n",
    "* Combine these columns with svd features and train additional model\n",
    "    * How does it perform ?\n",
    "\n",
    "![](https://i.ibb.co/jHR2hdZ/part-1-project-Page-2-drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Bonus** Use `KNeighborsClassifier` to build 10 models that use a few features from 3 to distinguish between 0 and 1.\n",
    "\n",
    "* How is the performance ?\n",
    "\n",
    "6. **Bonus** Use Convolution to reduce dimension of dataset to 18x18. \n",
    "\n",
    "> Hint: Google and use different kernels as you see fit.\n",
    "\n",
    "* Repeat all above with convoluted data. \n",
    "* Does convolution improve `f1-score` ? \n",
    "\n",
    "7. Also share me where you have burmese digit photos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import cluster\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
    "\n",
    "print('train_X: ' + str(train_X.shape))\n",
    "print('train_y: ' + str(train_y.shape))\n",
    "print('test_X:  '  + str(test_X.shape))\n",
    "print('test_y:  '  + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_k(X, k=30000):\n",
    "    return ??? # get first k samples/records from first dimension\n",
    "\n",
    "def flatten(X):\n",
    "    return ??? # reshape 2nd and 3rd dimensions into 1D \n",
    "\n",
    "def make_18_x_18(X):\n",
    "    return ??? # get the centre 18 x 18 records\n",
    "\n",
    "tr_X1 = make_18_x_18(train_X)\n",
    "tr_X1 = flatten(tr_X1)\n",
    "tr_X1 = get_k(tr_X1)\n",
    "ts_X1 = make_18_x_18(test_X)\n",
    "ts_X1 = flatten(ts_X1)\n",
    "ts_X1 = get_k(ts_X1)\n",
    "\n",
    "tr_y1 = get_k(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratios = [0.0, 0.2, 0.5, 0.8, 1.0]\n",
    "models_1 = []\n",
    "for l1_ratio in tqdm(l1_ratios):\n",
    "    model = linear_model.LogisticRegression(\n",
    "        penalty=\"elasticnet\", \n",
    "        C=1.0/???, # put a value not less than 500 and not more than 20000\n",
    "        l1_ratio=l1_ratio,\n",
    "        multi_class=\"ovr\",\n",
    "        solver=\"saga\",\n",
    "        random_state=42,\n",
    "        max_iter=???, # put a value less than 3000\n",
    "        n_jobs=4,\n",
    "        verbose=1\n",
    "    )\n",
    "    model.fit(tr_X1, tr_y1)\n",
    "\n",
    "    y_pred = model.predict(ts_X1)\n",
    "    y_pred_proba = model.predict_proba(ts_X1)\n",
    "    classification_report = metrics.classification_report(y_true=test_y, y_pred=y_pred)\n",
    "    models_1.append({\n",
    "        \"model\": model, \n",
    "        \"classification_report\": classification_report,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_pred_proba\": y_pred_proba\n",
    "    })\n",
    "    print (\"---\")\n",
    "    print (model)\n",
    "    print (classification_report)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open (\"model_set_1.bin\", \"wb\") as f:\n",
    "    pickle.dump(models_1, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_X2 = flatten(train_X)\n",
    "ts_X2 = flatten(test_X)\n",
    "\n",
    "tr_y2 = train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "Given a matrix $A$, which is an $m \\times n$ matrix, it can be approximated as:\n",
    "\n",
    "$$A_{\\text{approx.}} = U \\Sigma V^T$$\n",
    "\n",
    "where $U$ is an $n \\times k$ matrix, $\\Sigma$ is a $k \\times k$ matrix and $V^T$ is a $k \\times m$ matrix. \n",
    "\n",
    "Since $U$ and $V^T$ are **othonormal** (just trust me on this), $V^TV$ and $U^TU$ are identity matrices $I$. \n",
    "\n",
    "So, $$AV = U \\Sigma V^T V = U \\Sigma I = U \\Sigma$$\n",
    "\n",
    "Since $V$ is $m \\times k$ matrix and is constant, we can use $AV$ as our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import linalg\n",
    "svd_results = {}\n",
    "k = 10\n",
    "n_classes = 10\n",
    "pbar = tqdm(list(range(0, 10)))\n",
    "for y_value in pbar:\n",
    "    selector = tr_y2 == y_value\n",
    "    X_ = tr_X2[selector]\n",
    "    pbar.set_description(\"y_value : {} ; X_ shape : {}\".format(y_value, X_.shape))\n",
    "    U, s, Vt = linalg.svd(X_)\n",
    "    S = linalg.diagsvd(s[:k], k, k)\n",
    "    Vt_ = Vt[:k, :]\n",
    "    V_ = Vt_.T\n",
    "    svd_results[y_value] = {\n",
    "        \"U\": U,\n",
    "        \"Vt\": Vt,\n",
    "        \"s\" : S,\n",
    "        \"Vt_\": Vt_, \n",
    "        \"V_\": V_,\n",
    "        \"selector\": selector\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_value in range(0, 10):\n",
    "    utils.visualize(svd_results[y_value][\"Vt_\"][0:5].reshape(-1, 28, 28), y=[y_value]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_X = []\n",
    "pbar = tqdm(list(range(0, 10)))\n",
    "for y_value in pbar:\n",
    "    pbar.set_description(\"calculating features for : {}\".format(y_value))\n",
    "    svd_result = svd_results[y_value]\n",
    "    V_ = svd_result[\"V_\"]\n",
    "    X_ = tr_X2 @ V_\n",
    "    features_X.append(X_)\n",
    "print(\"concating all features ... \")\n",
    "tr_X3 = np.hstack(tuple(features_X))\n",
    "tr_y3 = train_y.copy()\n",
    "tr_X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = linear_model.LogisticRegression(\n",
    "    penalty=\"elasticnet\", \n",
    "    C=1.0/5000,\n",
    "    l1_ratio=0.2,\n",
    "    multi_class=\"ovr\",\n",
    "    solver=\"saga\",\n",
    "    random_state=42,\n",
    "    max_iter=5000,\n",
    "    n_jobs=4,\n",
    "    verbose=1\n",
    ")\n",
    "model_3.fit(tr_X3, tr_y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(X, svd_results):\n",
    "    return ??? # write a program to work with any X\n",
    "\n",
    "ts_X3 = get_features(test_X, svd_results=svd_results)\n",
    "ts_X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3.predict(ts_X3)\n",
    "y_pred_proba = model_3.predict_proba(ts_X3)\n",
    "classification_report = metrics.classification_report(y_true=test_y, y_pred=y_pred)\n",
    "\n",
    "print (\"---\")\n",
    "print (model_3)\n",
    "print (classification_report)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9c1e998c7d6d5f29587b2c70e9bd488bb486b902354401efc27ca8457f04e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
