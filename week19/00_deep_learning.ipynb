{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 19 - Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "မင်္ဂလာပါ။ Data Science Using Python - Week 19 က ကြိုဆိုပါတယ်။ \n",
    "\n",
    "ဒီအပါတ်မှာ ကျနော်တို့ **Deep Learning** ကို မိတ်ဆက်သဘောနဲ့ လေ့လာမှာ ဖြစ်ပါတယ်။ \n",
    "\n",
    "> Deep Learning ဟာ Machine Learning ရဲ့ အစိတ်အပိုင်းတခု၊ Machine Learning ဟာ Data Science ရဲ့ အစိတ်အပိုင်းတခုလို့ ဆိုပေမဲ့ Deep Learning နဲ့ ပတ်သက်တာအကုန်လုံးကို ဒီ course ထဲမှာ အကုန် ထည့်သွင်း သင်ကြားဖို့ လက်တွေ့မှာ မဖြစ်နိုင်ပါဘူး။\n",
    "\n",
    "Deep Learning အကြောင်းကို မပြောခင် Machine Learning technique တခုရဲ့ သဘောကို ပြန်နွှေး လေ့လာကြည့်ရအောင်။"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Machine Learning Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as sk_ds\n",
    "from sklearn import neural_network as sk_nn\n",
    "from sklearn import preprocessing as sk_pp\n",
    "from sklearn import model_selection as sk_ms\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. This is not ML; This is loading and preparing data\n",
    "df_X, ds_y = sk_ds.fetch_openml(\"credit-g\", as_frame=True, return_X_y=True)\n",
    "df_X_tr, df_X_ts, ds_y_tr, ds_y_ts = sk_ms.train_test_split(df_X, ds_y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature Definition \n",
    "\n",
    "ဒီအဆင့်မှာ ဘယ် feature transformation (အလုပ်ထဲတွင် feature ဟုပဲ လုံးချပြီး ပြောတတ်) တွေကို ဘယ်လို အဆင့်ဆင့် သုံးမယ်ဆိုတာကို စဉ်းစား သတ်မှတ် ရပါတယ်။\n",
    "\n",
    "> အရေးအကြီးဆုံးက test set ကို ဘယ်တော့မှ ထည့်ပြီး မ `fit` ဖို့ပါပဲ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Feature Definition\n",
    "def fit_features(X: pd.DataFrame):\n",
    "    X_num = X.select_dtypes(include=[\"number\"])\n",
    "    feature_1_ss = sk_pp.StandardScaler()\n",
    "    feature_2_pf = sk_pp.PolynomialFeatures(degree=2)\n",
    "    feature_1_ss.fit(X_num)\n",
    "    X_f1 = feature_1_ss.transform(X_num)\n",
    "    feature_2_pf.fit(X_f1)\n",
    "    return (feature_1_ss, feature_2_pf)\n",
    "\n",
    "feature_definitions = fit_features(df_X_tr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Definition\n",
    "\n",
    "ဒီအဆင့်မှာတော့ ဘယ် model definition (အလုပ်ထဲတွင် model ဟုပဲ လုံးချပြီး ပြောတတ်) ကို သုံးမယ်ဆိုတာကို စဉ်းစား သတ်မှတ်ရပါတယ်။ \n",
    "\n",
    "> course တခုလုံးမှာ init လို့ ပြောခဲ့တဲ့ အဆင့်ပါ။\n",
    "\n",
    "1. ဒီလို စဉ်းစားရာမှာ linear model, svm, random forest, gradient boosting trees သို့မဟုတ် neural network စသဖြင့် စဉ်းစားရတာက ပထမအဆင့် ဖြစ်ပြီး\n",
    "   \n",
    "2. ဒုတိယအဆင့်အနေနဲ့ တခုချင်းစီရဲ့ hyper-parameter တွေကို သတ်မှတ်ပေးရပါတယ်။ \n",
    "   \n",
    "   > ဥပမာအားဖြင့် `sklearn.linear_models.SGDClassifier` မှာဆိုရင် hyper-parameter အနေနဲ့ `l1_ratio`, `penalty` စတာတွေကို သတ်မှတ်ပေးရပါတယ်။ \n",
    "   အဲဒီကမှ model-parameter (`coef_` နဲ့ `intercept_` တို့) ကို learn ယူတာ ဖြစ်ပါတယ်။\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Definition - What model **architecture** to use\n",
    "model_nn = sk_nn.MLPClassifier(\n",
    "    # architecture of neural network\n",
    "    hidden_layer_sizes=(20, 20), activation=\"relu\", \n",
    "    # optimizer SGD, ADAM or whatever fancy one\n",
    "    solver=\"adam\", \n",
    "    # steps in train loop characteristics\n",
    "    batch_size=100, learning_rate=\"constant\", learning_rate_init=0.001, max_iter=10000, shuffle=True, random_state=42,\n",
    "    # other model parameters\n",
    "    alpha=0.0001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Computation\n",
    "\n",
    "ဒီအဆင့်မှာ စောစောက သတ်မှတ်ထားတဲ့ feature definition တွေအတိုင်း feature value တွေ (အလုပ်ထဲမှာ feature လို့ပဲ လုံးချပြီး ပြော) ကို compute လုပ်ရတာ ဖြစ်ပါတယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Computation\n",
    "def transform_features(X: pd.DataFrame, features=()):\n",
    "    X_num = X.select_dtypes(include=[\"number\"])\n",
    "    _features = X_num\n",
    "    for f in features:\n",
    "        _features = f.transform(_features)\n",
    "    return _features\n",
    "\n",
    "feature_values_tr = transform_features(df_X_tr, features=feature_definitions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training\n",
    "\n",
    "ဒီအဆင့်မှာတော့ အဆင့် ၃ က ရထားတဲ့ feature values တွေကို အဆင့် ၂ မှာ သတ်မှတ်ထားတဲ့ model definition နဲ့ train (`fit`) တာ ဖြစ်ပါတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model Training\n",
    "\n",
    "def fit_model(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "model_nn = fit_model(model_nn, feature_values_tr, ds_y_tr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Validation\n",
    "\n",
    "ဒုတိယ နောက်ဆုံးအဆင့်ဖြစ်တဲ့ ဒီအဆင့်မှာတော့ ရလာတဲ့ trained model ကို စာမေးပွဲစစ်သလို unseen data နဲ့ စစ်ဆေးတဲ့ သဘောဖြစ်ပါတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Model Validation\n",
    "\n",
    "def validate_model(features, model, X_ts, y_ts):\n",
    "    feature_values_ts = transform_features(X_ts, features=features)\n",
    "    y_hat = model.predict(feature_values_ts)\n",
    "    print (sk_metrics.classification_report(y_ts, y_hat, sample_weight=[{\"good\": 1, \"bad\": 5}[_y] for _y in y_ts]))\n",
    "    return feature_values_ts\n",
    "\n",
    "feature_values_ts = validate_model(features=feature_definitions, model=model_nn, X_ts=df_X_ts, y_ts=ds_y_ts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Model Application\n",
    "\n",
    "Model Scoring/Inferencing လို့လဲ ခေါ်ကြတဲ့ ဒီ နောက်ဆုံး အဆင့်ကတော့ validation လုပ်လို့ ကျေနပ်မှ (model စာမေးပွဲအောင်မှ) ဆက်လုပ်ကြတာပါ။\n",
    "\n",
    "> Validation မှာ performance မကောင်းရင် ဘာဆက်လုပ်ကြမလဲ ??? \n",
    "\n",
    "စောစောက feature definition တွေနဲ့ model definition တွေကို production ကို ယူသွားပြီး data အသစ်တွေကို apply လုပ်ကြပါတယ်။ \n",
    "\n",
    "```python\n",
    "# how to move features and models to production ?\n",
    "\n",
    "# 1. pickle them in training machine\n",
    "\n",
    "import pickle\n",
    "with open(\"feats_and_model.bin\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"features\": feature_definitions, \n",
    "        \"model\": model_nn\n",
    "    }, f)\n",
    "\n",
    "# 2. un-pickle them in production machine\n",
    "\n",
    "import pickle\n",
    "with open(\"feats_and_model.bin\", \"rb\") as f:\n",
    "    _fm = pickle.load(f)\n",
    "    feature_definitions = _fm[\"features\"]\n",
    "    model_nn = _fm[\"model\"]\n",
    "\n",
    "# use them with new data \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "အကျဉ်းချုပ် အနေနဲ့ Machine Learning Application တခုရဲ့ structure ဟာ ဒီလို လာပါတယ်။\n",
    "\n",
    "```python\n",
    "df_X_tr, df_X_ts, ds_y_tr, ds_y_ts = load_and_split_data()\n",
    "\n",
    "feature_def = get_feature_definition()\n",
    "feature_def.fit(df_X_tr)\n",
    "\n",
    "model_def = get_model_definition(\n",
    "    model_architecture,\n",
    "    model_optimizer,\n",
    "    model_step\n",
    ")\n",
    "\n",
    "feature_values_tr = feature_def.transform(df_X_tr)\n",
    "model_def.fit(feature_values_tr, ds_y_tr)\n",
    "\n",
    "feature_values_ts = feature_def.transform(df_X_ts)\n",
    "y_hat = model_def.predict(feature_values_ts)\n",
    "print (classification_report(ds_y_ts, y_hat))\n",
    "```\n",
    "\n",
    "> အရေးပါတဲ့ မှတ်သားစရာတွေက \n",
    "> * `model_def` မှာ `model_architecture`, `model_optimizer` နဲ့ `model_step` ဆိုပြီး ၃ ပိုင်းပါပါတယ်။\n",
    "> * trained model ရဲ့ performance ပေါ်မူတည်ပြီး `feature_def` ကို သွားသွားပြန်ပြောင်းပြီး စမ်းဖို့ လိုပါတယ်။ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Deep Learning ဟာ hidden layer 1 ခုထက် ပိုပါတဲ့ Neural Network ပဲ ဖြစ်တယ်။ \n",
    "\n",
    "Computing efficiency ကြောင့်ပဲလို့ ဆိုဆို၊ ဘာကြောင့်ပဲလို့ ဆိုဆို width ရော၊ breadth ရော၊ depth ရော ကြီးတဲ့ neural network model တွေအနေနဲ့ `sklearn.neural_network` ထဲက model တွေလိုမျိုး **fully connected** ဖြစ်လို့ အဆင်မပြေပါဘူး။ \n",
    "\n",
    "ဒါကြောင့် neural network ရဲ့ definition (အလုပ်ထဲတွင် architecture ဟု ပြော) ကို စိတ်ကြိုက် ပြောင်းပေးနိုင်ဖို့ လိုလာပါတယ်။ \n",
    "\n",
    "အဲဒါကို လုပ်ပေးနိုင်တဲ့ library ၂ ခု ရှိပါတယ်။ \n",
    "\n",
    "* tensorflow by Google and\n",
    "* torch by Facebook \n",
    "\n",
    "တို့ ဖြစ်ပါတယ်။ \n",
    "\n",
    "tensorflow က powerful ပိုဖြစ်ပြီး torch (pytorch) က လေ့လာရတာ ပိုလွယ်ပါတယ်။ ဒါကြောင့် ဒီနေ့ `torch` ကို စမ်းကြည့်ကြပါမယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အောက်က model definition နဲ့ တူတဲ့ model တခုကို ဆောက်ကြည့်ရအောင်။\n",
    "\n",
    "```python\n",
    "# 2. Model Definition - What model **architecture** to use\n",
    "model_nn = sk_nn.MLPClassifier(\n",
    "    # architecture of neural network\n",
    "    hidden_layer_sizes=(20, 20), activation=\"relu\", \n",
    "    # optimizer SGD, ADAM or whatever fancy one\n",
    "    solver=\"adam\", \n",
    "    # steps in train loop characteristics\n",
    "    batch_size=100, learning_rate=\"constant\", learning_rate_init=0.001, max_iter=10000, shuffle=True, random_state=42,\n",
    "    # other model parameters\n",
    "    alpha=0.0001)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = feature_definitions[-1].n_output_features_ # the number of columns in X\n",
    "hidden_layer_sizes = (20, 20)\n",
    "output_layer_size = 2 # it has good and bad (2 classes)\n",
    "\n",
    "# architecture of neural network (including activation)\n",
    "class TorchModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TorchModel, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(input_layer_size, hidden_layer_sizes[0])\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(hidden_layer_sizes[0], hidden_layer_sizes[1])\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.output = torch.nn.Linear(hidden_layer_sizes[1], output_layer_size)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_1 = self.relu1(self.layer1(x))\n",
    "        x_2 = self.relu2(self.layer2(x_1))\n",
    "        x_3 = self.relu3(self.output(x_2))\n",
    "        return x_3\n",
    "\n",
    "model_t = TorchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate_init=0.001\n",
    "solver = torch.optim.Adam(model_t.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "X_tr_tensor = torch.Tensor(feature_values_tr)\n",
    "X_ts_tensor = torch.Tensor(feature_values_ts)\n",
    "y_tr_tensor = torch.Tensor([{\"good\": 0, \"bad\": 1}[_y] for _y in ds_y_tr]).long()\n",
    "y_ts_tensor = torch.Tensor([{\"good\": 0, \"bad\": 1}[_y] for _y in ds_y_ts]).long()\n",
    "\n",
    "tr_dataset = TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop -- including model train step\n",
    "max_iter=10000\n",
    "\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(max_iter):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero-set the parameter gradients; important\n",
    "        solver.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model_t(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        solver.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    if epoch % 2000 == 1999:    # print every 2000 epoch\n",
    "        print('epoch = {epoch} : loss = {running_loss}'.format(epoch=epoch, running_loss=running_loss/2000.0))\n",
    "        running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_t(X_ts_tensor)\n",
    "y_predicted = [0 if y_[0] > y_[1] else 1 for y_ in F.softmax(y_hat).long()]\n",
    "print (sk_metrics.classification_report(y_ts_tensor.detach(), y_predicted, sample_weight=[{\"good\":1, \"bad\": 5}[_y] for _y in ds_y_ts]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Analysis of Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` package များနှင့် deep learning (`torch`) တို့၏ ကွဲပြားခြားနားချက်များမှာ \n",
    "\n",
    "* model architecture, အတွင်းပိုင်း ချိတ်ဆက်ပုံနှင့် အဖြေထုတ်ပုံတို့ကို deep learning တွင် user ကိုယ်တိုင် သတ်မှတ်ရခြင်း ဖြစ်သည်။ `sklearn` တွင်မူ model များ၏ အလုပ်လုပ်ပုံမှာ အသေဖြစ်ပြီး hyper parameter များမှတဆင့် အနည်းငယ်သာ ပြောင်းလဲ၍ ရသည်။\n",
    "\n",
    "* Feature Engineering ပိုင်းအတွက် မည်သည့် Feature Function ကို သုံးမည်၊ မည်သို့ ချိတ်ဆက်မည်ကို deep learning တွင် အလိုအလျောက် learn နိုင်သည်။ layer များများ ထည့်ပြီး compute resources များများ သုံးရန်သာ လိုသည်။\n",
    "\n",
    "* ပိုမို မြန်ဆန်အောင် GPU သုံး၍ ရသည်။ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> အခု ကျနော်တို့ GPU သုံးတာ လက်တွေ့ ပြမှာမို့လို့ Runtime ကို restart လုပ်ပေးပါ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as sk_ds \n",
    "from sklearn import model_selection as sk_ms\n",
    "from sklearn import preprocessing as sk_pp\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = sk_ds.fetch_openml(\"credit-g\", as_frame=True, return_X_y=True)\n",
    "\n",
    "df_X_tr, df_X_ts, ds_y_tr, ds_y_ts = sk_ms.train_test_split(df_X, ds_y, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "ohe = sk_pp.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe.fit(df_X_tr.select_dtypes(include=[\"category\"]))\n",
    "X_tr = pd.DataFrame(data=ohe.transform(df_X_tr.select_dtypes(include=[\"category\"])), index=df_X_tr.index, columns=ohe.get_feature_names_out())\n",
    "X_ts = pd.DataFrame(data=ohe.transform(df_X_ts[ohe.feature_names_in_]), index=df_X_ts.index, columns=ohe.get_feature_names_out())\n",
    "\n",
    "X_tr = pd.concat((X_tr, df_X_tr.select_dtypes(include=[\"number\"])), axis=\"columns\")\n",
    "X_ts = pd.concat((X_ts, df_X_ts.select_dtypes(include=[\"number\"])), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size = X_tr.shape[1]\n",
    "output_layer_size = 2\n",
    "class TorchModel0(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TorchModel0, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_layer_size, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 20),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(20, 2),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model_t0 = TorchModel0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate_init=0.001\n",
    "solver0 = torch.optim.SGD(params=model_t0.parameters(), lr=learning_rate_init, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "X_tr_tensor = torch.Tensor(X_tr.values).to(device)\n",
    "X_ts_tensor = torch.Tensor(X_ts.values).to(device)\n",
    "y_tr_tensor = torch.Tensor([{\"good\": 0, \"bad\": 1}[_y] for _y in ds_y_tr]).long().to(device)\n",
    "y_ts_tensor = torch.Tensor([{\"good\": 0, \"bad\": 1}[_y] for _y in ds_y_ts]).long().to(device)\n",
    "\n",
    "tr_dataset = TensorDataset(X_tr_tensor, y_tr_tensor)\n",
    "\n",
    "train_loader = DataLoader(tr_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop -- including model train step\n",
    "def train_model(max_iter=10000, model_t=None, solver=None, loss_func=None):\n",
    "\n",
    "    model_t = model_t.to(device)\n",
    "\n",
    "    for epoch in range(max_iter):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero-set the parameter gradients; important\n",
    "            solver.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model_t(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            loss.backward()\n",
    "            solver.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        if epoch % 2000 == 1999:    # print every 2000 epoch\n",
    "            print('epoch = {epoch} : loss = {running_loss}'.format(epoch=epoch, running_loss=running_loss/2000.0))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "    return model_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t0 = train_model(model_t=model_t0, solver=solver0, loss_func = torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t0.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model_t0(X_ts_tensor)\n",
    "    y_predicted = [0 if y_[0] > y_[1] else 1 for y_ in F.softmax(y_hat).long()]\n",
    "    print (sk_metrics.classification_report(y_ts_tensor.detach().cpu(), y_predicted, sample_weight=[{\"good\":1, \"bad\": 5}[_y] for _y in ds_y_ts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture of neural network (including activation)\n",
    "input_layer_size = X_tr.shape[1]\n",
    "output_layer_size = 2\n",
    "class TorchModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TorchModel, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.BatchNorm1d(input_layer_size),\n",
    "            torch.nn.Linear(input_layer_size, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm1d(100),\n",
    "            torch.nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(100, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(50, 10),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(10, output_layer_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model_t = TorchModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "learning_rate_init=0.001\n",
    "solver = torch.optim.SGD(params=model_t.parameters(), lr=learning_rate_init, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t = train_model(model_t=model_t, solver=solver, loss_func = torch.nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model_t(X_ts_tensor)\n",
    "    y_predicted = [0 if y_[0] > y_[1] else 1 for y_ in F.softmax(y_hat).long()]\n",
    "    print (sk_metrics.classification_report(y_ts_tensor.detach().cpu(), y_predicted, sample_weight=[{\"good\":1, \"bad\": 5}[_y] for _y in ds_y_ts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-dsup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
