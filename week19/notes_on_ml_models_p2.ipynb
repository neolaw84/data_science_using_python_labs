{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Machine Learning Models - Part 2 - Model Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အပိုင်း ၃ ပိုင်းရှိတဲ့ ဒီ notes တွေမှာ အဓိကအားဖြင့် Machine Learning Model တွေကို အသုံးပြုရာမှာ သတိပြုရမဲ့ အောက်ပါ အကြောင်းအရာများကို ပြောပြပေးသွားမှာ ဖြစ်ပါတယ်။ \n",
    "\n",
    "* Feature Engineering \n",
    "* Model Engineering and\n",
    "* Coding Best Practices\n",
    "\n",
    "ဒီအပိုင်းက ဒုတိယအပိုင်း Model Engineering ဖြစ်ပါတယ်။ \n",
    "\n",
    "Model Engineering အပိုင်းမှာ ပထမဆုံး ဆုံးဖြတ်ရမှာကတော့ လက်ရှိ problem ဟာ ဘယ်အမျိုးအစားလဲ ဆိုတာပဲ ဖြစ်ပါတယ်။\n",
    "\n",
    "Machine Learning problem တွေကို အောက်ပါအတိုင်း အမျိုးအစား ခွဲခြားနိုင်ပါတယ်။ \n",
    "\n",
    "* Supervised Learning \n",
    "  * Regression\n",
    "  * Classification\n",
    "* Unsupervised Learning\n",
    "  * Clustering\n",
    "  * Dimensionality Reduction\n",
    "* Semi-supervised Learning\n",
    "  * Reinforcement Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised Learning**\n",
    "\n",
    "Supervised Learning မှာ \n",
    "\n",
    "* Given $X$ and $y$ တွေကနေပြီး သူတို့ရဲ့ ဆက်သွယ်ချက် function $f(X)=\\hat{y}$ ကို ရှာတာကို training\n",
    "* Unseen $X_{\\mathrm{unseen}}$ တွေနဲ့ $f(X)$ ကနေပြီး ဖြစ်နိုင်တဲ့ $\\hat{y}_{\\mathrm{unseen}}$ ကို တွက်တဲ့ inference ဆိုပြီး ရှိပါတယ်။ \n",
    "\n",
    "Training လုပ်ရာမှာ unseen ကို simulate လုပ်တဲ့အနေနဲ့ dataset (both $X$ and $y$) ကို train/test split လုပ်လေ့ရှိပါတယ်။ \n",
    "\n",
    "> **အရေးကြီး မှတ်ရန်မှာ train လုပ်နေစဉ်အတွင်း ဘယ်တော့မှ test split နဲ့ `fit` မလုပ်ရန် ဖြစ်သည်**\n",
    "\n",
    "ဒီနေရာမှာ $y$ ရဲ့ data type ပေါ်မူတည်ပြီး \n",
    "\n",
    "* real numeric value (float) ဖြစ်ခဲ့ရင် regression\n",
    "* categorical value (int/str) ဖြစ်ခဲ့ရင် classification လို့ ခွဲတာ ဖြစ်ပါတယ်။\n",
    "\n",
    "ဒီအပိုင်းမှာ \n",
    "\n",
    "* Model Validation Technique တွေနဲ့\n",
    "* Bias and Variance ကို Quantify လုပ်တာ တို့ကို ပြောမှာ ဖြစ်ပါတယ်။ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsupervised Learning**\n",
    "\n",
    "Unsupervised Learning မှာ \n",
    "\n",
    "* ပေးထားတဲ့ data ကို တူရာတူရာ စုတာကို clustering နဲ့ \n",
    "* dimension (column) တွေ များနေတာကို (information မလျော့စေပဲ) နည်းအောင်လုပ်တာကို dimensionality reduction လို့ ၂ မျိုး ခွဲခြားနိုင်ပါတယ်။\n",
    "\n",
    "ဒီနေ့ခေတ် ပြဿနာအများစုက supervised learning ဖြစ်ပြီး unsupervised learning တွေကို supervised learning solution တွေရဲ့ feature engineering အဖြစ် သုံးတတ်ကြပါတယ်။ \n",
    "\n",
    "> sklearn က clustering/dimensionality-reduction package တွေကို အသုံးပြုနည်းကို Part 1 - Feature Engineering မှာ အသေးစိတ် ပြောထားပြီးပြီမို့လို့ ဒီအပိုင်းမှာ ပြန်မပြောတော့ပါ။ \n",
    "\n",
    "ဒီအပိုင်းမှာ \n",
    "\n",
    "* cosine distance ကို Euclidean distance ပဲ အသုံးပြုလို့ရတဲ့ clustering algorithm တွေမှာ ဘယ်လို သုံးရလဲဆိုတာ\n",
    "\n",
    "ကို ဥပမာ ပြပြီး ပြောမှာ ဖြစ်ပါတယ်။"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-supervised Learning**\n",
    "\n",
    "Semi-supervised Learning မှာတော့ ရည်မှန်းချက် ပန်းတိုင် (ကားမောင်းတာ၊ ဂိမ်းဆော့တာ စသည်) တခုကို ပေးထားပြီး အဲဒီ ပန်းတိုင်ကို ရောက်အောင် လုပ်ရမဲ့ strategy ကို learn လုပ်တာတွေဖြစ်တဲ့ reinforcement learning ပါပါတယ်။ \n",
    "\n",
    "> advance topic ဖြစ်တဲ့အတွက်ရယ်၊ လက်တွေ့ လုပ်ငန်းခွင် application အများအပြား မရှိသေးတဲ့အတွက်ရယ် ဒီအပိုင်းမှာ မပြောတော့ပါဘူး။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Preparations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Validation မစခင်မှာ ကြိုတင် ပြင်ဆင်တဲ့ အနေနဲ့ Part 1 က Feature တွေကို အရင် တွက်ကြမယ်။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as sk_pp\n",
    "from sklearn import datasets as sk_ds\n",
    "from sklearn import model_selection as sk_ms\n",
    "from sklearn import feature_selection as sk_fs\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = sk_ds.fetch_openml(name=\"credit-g\", as_frame=True, return_X_y=True)\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_tr, df_X_ts, ds_y_tr, ds_y_ts = sk_ms.train_test_split(df_X, ds_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_tr_num = df_X_tr.select_dtypes(include=[\"number\"])\n",
    "numeric_columns = df_X_tr_num.columns\n",
    "\n",
    "categorical_columns = df_X_tr.select_dtypes(include=[\"category\"]).columns\n",
    "ordinal_columns = [\"credit_history\", \"savings_status\"]\n",
    "norminal_columns = [c for c in categorical_columns if c not in ordinal_columns]\n",
    "\n",
    "oe = sk_pp.OrdinalEncoder(\n",
    "    # အောက်က categories parameter မှာ array-like of array-like (list of list) ထည့်ပေးရတာ သတိပြုပါ။\n",
    "    categories=[\n",
    "        ['no credits/all paid', 'all paid', 'existing paid', 'delayed previously', 'critical/other existing credit'],\n",
    "        ['no known savings', '<100', '100<=X<500', '500<=X<1000', '>=1000']\n",
    "    ], \n",
    "    # handle_unknown က သိပ်အရေးကြီးတယ်။ ဒါမပါသွားရင် production ကျမှ ပြဿနာ တက်တတ်တယ်။ default is \"error\"\n",
    "    handle_unknown=\"use_encoded_value\", \n",
    "    unknown_value=np.nan\n",
    ")\n",
    "oe.fit(df_X_tr[ordinal_columns])\n",
    "ordinal_features = [\"oe_{}\".format(c) for c in ordinal_columns]\n",
    "\n",
    "df_X_tr_num.loc[:, ordinal_columns] = oe.transform(df_X_tr[ordinal_columns])\n",
    "\n",
    "ohe = sk_pp.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "ohe.fit(df_X_tr[norminal_columns])\n",
    "\n",
    "ss = sk_pp.StandardScaler()\n",
    "mms = sk_pp.MinMaxScaler()\n",
    "mas = sk_pp.MaxAbsScaler()\n",
    "\n",
    "numerical_columns = df_X_tr_num.columns\n",
    "\n",
    "norminal_features = ohe.get_feature_names_out()\n",
    "df_feat_tr = pd.DataFrame(data=None, index=df_X_tr.index)\n",
    "df_feat_tr.loc[:, norminal_features] = ohe.transform(df_X_tr[norminal_columns])\n",
    "\n",
    "ss_features = [\"ss_{}\".format(c) for c in numerical_columns]\n",
    "mms_features = [\"mms_{}\".format(c) for c in numerical_columns]\n",
    "mas_features = [\"mas_{}\".format(c) for c in numerical_columns]\n",
    "\n",
    "df_feat_tr.loc[:, ss_features] = ss.fit_transform(df_X_tr_num)\n",
    "df_feat_tr.loc[:, mms_features] = mms.fit_transform(df_X_tr_num)\n",
    "df_feat_tr.loc[:, mas_features] = mas.fit_transform(df_X_tr_num)\n",
    "\n",
    "df_feat_tr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အပေါ်က cell မှာ ပါတာတွေက part 1 မှာ ရှင်းပြီးသားမို့ အသေးစိတ် မရှင်းတော့ဘူး။ \n",
    "\n",
    "> ရှေ့မှာ သုံးလို့ လွယ်အောင် အပေါ်က cell မှာ လုပ်ထားတာတွေကို function 2 ခု ခွဲရေးလိုက်မယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_features(X, ordinal_columns=[\"credit_history\", \"savings_status\"]):\n",
    "    X_num = X.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    categorical_columns = X.select_dtypes(include=[\"category\"]).columns\n",
    "    norminal_columns = [c for c in categorical_columns if c not in ordinal_columns]\n",
    "\n",
    "    oe = sk_pp.OrdinalEncoder(\n",
    "        categories=[\n",
    "            ['no credits/all paid', 'all paid', 'existing paid', 'delayed previously', 'critical/other existing credit'],\n",
    "            ['no known savings', '<100', '100<=X<500', '500<=X<1000', '>=1000']\n",
    "        ], \n",
    "        handle_unknown=\"use_encoded_value\", \n",
    "        unknown_value=np.nan\n",
    "    )\n",
    "    oe.fit(X[ordinal_columns])\n",
    "    \n",
    "    X_num.loc[:, ordinal_columns] = oe.transform(X[ordinal_columns])\n",
    "\n",
    "    ohe = sk_pp.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n",
    "    ohe.fit(X[norminal_columns])\n",
    "    \n",
    "    ss, mms, mas = sk_pp.StandardScaler(), sk_pp.MinMaxScaler(), sk_pp.MaxAbsScaler()\n",
    "\n",
    "    ss.fit(X_num), mms.fit(X_num), mas.fit(X_num)\n",
    "\n",
    "    return oe, ohe, ss, mms, mas\n",
    "\n",
    "def transform_features(X, oe, ohe, ss, mms, mas):\n",
    "    X_num = X.select_dtypes(include=[\"number\"])\n",
    "    X_num.loc[:, ordinal_columns] = oe.transform(X[ordinal_columns])\n",
    "\n",
    "    categorical_columns = X.select_dtypes(include=[\"category\"]).columns\n",
    "    norminal_columns = [c for c in categorical_columns if c not in ordinal_columns]\n",
    "\n",
    "    df_feat_tr = pd.DataFrame(data=None, index=X.index)\n",
    "    \n",
    "    norminal_features = ohe.get_feature_names_out()\n",
    "    df_feat_tr.loc[:, norminal_features] = ohe.transform(X[norminal_columns])\n",
    "\n",
    "    numerical_columns = X_num.columns\n",
    "\n",
    "    ss_features = [\"ss_{}\".format(c) for c in numerical_columns]\n",
    "    mms_features = [\"mms_{}\".format(c) for c in numerical_columns]\n",
    "    mas_features = [\"mas_{}\".format(c) for c in numerical_columns]\n",
    "\n",
    "    df_feat_tr.loc[:, ss_features] = ss.transform(X_num)\n",
    "    df_feat_tr.loc[:, mms_features] = mms.transform(X_num)\n",
    "    df_feat_tr.loc[:, mas_features] = mas.transform(X_num)\n",
    "\n",
    "    return df_feat_tr\n",
    "\n",
    "oe, ohe, ss, mms, mas = fit_features(df_X_tr)\n",
    "df_feat_tr = transform_features(df_X_tr, oe, ohe, ss, mms, mas)\n",
    "df_feat_tr.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA နဲ့ SVD Features တွေကိုတော့ ထည့်မပြတော့ဘူး။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = sk_pp.LabelEncoder()\n",
    "y_tr = le.fit_transform(ds_y_tr)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts = le.transform(ds_y_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = 1 - y_tr\n",
    "y_ts = 1 - y_ts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model တခုရဲ့ Performance ကို တိုင်းတာတဲ့ နည်းတွေ အများကြီးရှိတယ်။ အဲဒီအထဲက အောက်ပါတို့ကို အသုံးများတယ်။ \n",
    "\n",
    "* Regression:\n",
    "  * Means Absolute Error (MAE)\n",
    "  * Means Absolute Percentage Error (MAPE)\n",
    "  * Means Squared Error (MSE)\n",
    "\n",
    "* Classification:\n",
    "  * Precision - TP/(TP+FP)\n",
    "  * Recall or Sensitivity - TP/(TP+FN)\n",
    "  * Specificity - TN/(TN+FN) \n",
    "  * F1 score - geometric means (or harmonic means) of Precision and Recall\n",
    "  * AUC ROC\n",
    "\n",
    "အခု problem က classification problem ဖြစ်တဲ့အတွက် Precision, Recall, AUC ROC တို့ကို တွက်မယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as sk_metrics\n",
    "from sklearn import linear_model as sk_lm\n",
    "from sklearn import svm as sk_svm\n",
    "from sklearn import ensemble as sk_ensemble\n",
    "from sklearn import neural_network as sk_nn\n",
    "from sklearn import neighbors as sk_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import traceback\n",
    "class_weight={0: 1, 1: 5}\n",
    "\n",
    "def fit_model(X, y, model):\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "def test_model(X, y, model, model_desc):\n",
    "    sample_weight = [class_weight[_y] for _y in y]\n",
    "    y_hat = model.predict(X)\n",
    "    try:\n",
    "        y_hat_probs = model.predict_proba(X)\n",
    "        if model_desc: print (model_desc) \n",
    "        print (sk_metrics.classification_report(y, y_hat, sample_weight=sample_weight))\n",
    "        print (\"roc auc : {}\".format(sk_metrics.roc_auc_score(y, y_hat_probs[:, 1], sample_weight=sample_weight)))\n",
    "        fpr, tpr, thr = sk_metrics.roc_curve(y, y_hat_probs[:, 1], sample_weight=sample_weight)\n",
    "        plt.plot(fpr, tpr)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        traceback.print_tb(e.__traceback__)\n",
    "\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = sk_lm.SGDClassifier(penalty=\"elasticnet\", loss=\"log\", max_iter=2000, n_jobs=2, random_state=42)\n",
    "svc = sk_svm.NuSVC(kernel=\"rbf\", class_weight=class_weight, random_state=42, probability=True)\n",
    "gbc = sk_ensemble.GradientBoostingClassifier(n_estimators=100, max_depth=64, max_leaf_nodes=1024, random_state=42)\n",
    "neighbors = sk_neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=2)\n",
    "nn = sk_nn.MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"relu\", solver=\"adam\", max_iter=2000, random_state=42)\n",
    "\n",
    "fit_model(df_feat_tr, y_tr, lm)\n",
    "fit_model(df_feat_tr, y_tr, svc)\n",
    "fit_model(df_feat_tr, y_tr, gbc)\n",
    "fit_model(df_feat_tr, y_tr, neighbors)\n",
    "fit_model(df_feat_tr, y_tr, nn)\n",
    "\n",
    "df_feat_ts = transform_features(df_X_ts, oe, ohe, ss, mms, mas)\n",
    "\n",
    "for model, model_desc in zip([lm, svc, gbc, neighbors, nn], [\"linear\", \"svm\", \"boost\", \"neighbors\", \"neural net\"]):\n",
    "    test_model(df_feat_ts, y_ts, model=model, model_desc=model_desc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Model Confidence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Performance ကို တိုင်းတာတော့ ဟုတ်ပါပြီ။ ကံကောင်းပြီး (ကြက်ကန်းတိုးပြီး) ဒီ performance ကို ရလာတာ မဟုတ်ဘူးလို့ ဘယ်လို သက်သေပြမလဲ။"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = sk_ms.KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "accuracies = []\n",
    "\n",
    "# notice how kfold can be used to get train_idx and test_idx\n",
    "for train_idx, test_idx in kfold.split(df_X, ds_y):\n",
    "    X_train, y_train = df_X.iloc[train_idx, :], ds_y.iloc[train_idx]\n",
    "    X_test, y_test = df_X.iloc[test_idx, :], ds_y.iloc[test_idx]\n",
    "\n",
    "    # transform the y because we haven't\n",
    "    _y_train = 1 - le.transform(y_train)\n",
    "    _y_test = 1 - le.transform(y_test)\n",
    "\n",
    "    # features\n",
    "    oe, ohe, ss, mms, mas = fit_features(X_train)\n",
    "    _X_train = transform_features(X_train, oe, ohe, ss, mms, mas)\n",
    "    _X_test = transform_features(X_test, oe, ohe, ss, mms, mas)\n",
    "    \n",
    "    # model\n",
    "    svc = sk_svm.NuSVC(kernel=\"rbf\", class_weight=class_weight, random_state=42, probability=True)\n",
    "    fit_model(_X_train, _y_train, svc)\n",
    "    \n",
    "    y_hat = test_model(_X_test, _y_test, model=svc, model_desc=\"svc\")\n",
    "    \n",
    "    accuracies.append((y_hat == _y_test).sum()/float(len(_y_test)))\n",
    "\n",
    "accuracies = np.array(accuracies)\n",
    "print(accuracies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence of Model Performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically speaking, ဘယ် data array ကို မဆို confidence interval တွက်လို့ ရတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "#create 95% confidence interval for population mean weight\n",
    "p=0.05\n",
    "st.t.interval(alpha=1 - p, df=len(accuracies)-1, loc=np.mean(accuracies), scale=st.sem(accuracies)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီပုံစံအတိုင်းပဲ precision/recall စတာတွေကိုလဲ confidence interval နဲ့ ဖော်ပြနိုင်တယ်။ "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance Trade-off"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ပထမဆုံး Bias များတဲ့ Model တခုနဲ့ Variance များတဲ့ Model တခုကို ဖန်တီးကြည့်ရအောင်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with bad_features, the model we create will be high variance (overfit)\n",
    "bad_features = [c for c in df_feat_tr.columns if c not in [\n",
    "    'checking_status_0<=X<200', 'checking_status_<0',\n",
    "    'checking_status_>=200', 'purpose_new car',\n",
    "    'personal_status_male div/sep', 'property_magnitude_car',\n",
    "    'job_skilled', 'ss_existing_credits', 'mms_credit_amount',\n",
    "    'mms_credit_history'\n",
    "    ]\n",
    "]\n",
    "high_variance = sk_neighbors.KNeighborsClassifier(n_neighbors=5, n_jobs=2)\n",
    "high_variance.fit(df_feat_tr[bad_features], y_tr)\n",
    "y_hat_high_variance = high_variance.predict(df_feat_ts[bad_features])\n",
    "print (sk_metrics.classification_report(y_ts, y_hat_high_variance, sample_weight=[{0:1, 1:5}[_y] for _y in y_ts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with some good features, the model we create will be ok\n",
    "# but we will tweak the intercept_ (the bias term) to make it bad\n",
    "good_features = ['checking_status_0<=X<200', 'checking_status_<0',\n",
    "    'checking_status_>=200', 'purpose_new car',\n",
    "    'mms_credit_amount','mms_credit_history'\n",
    "]\n",
    "high_bias = sk_lm.SGDClassifier(loss=\"log\", penalty=\"elasticnet\", max_iter=500, n_jobs=2)\n",
    "high_bias.fit(df_feat_tr[good_features], y_tr)\n",
    "high_bias.intercept_ = high_bias.intercept_ - 0.2\n",
    "y_hat_high_bias = high_bias.predict(df_feat_ts[good_features])\n",
    "print (sk_metrics.classification_report(y_ts, y_hat_high_bias, sample_weight=[{0:1, 1:5}[_y] for _y in y_ts]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION** -- How to differentiate them ?\n",
    "\n",
    "မြင်နေရတဲ့ classification report မှာ accuracy က ထပ်တူထပ်မျှ ညံ့နေတယ်။ ဒီတော့ model က bias များနေသလား၊ variance များနေသလား ဘယ်လို သိနိုင်မလဲ။"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER** -- Training Dataset ကို Predict လုပ်ခိုင်းကြည့်ပါ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_high_variance_tr= high_variance.predict(df_feat_tr[bad_features])\n",
    "print (sk_metrics.classification_report(y_tr, y_hat_high_variance_tr, sample_weight=[{0:1, 1:5}[_y] for _y in y_tr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_high_bias_tr = high_bias.predict(df_feat_tr[good_features])\n",
    "print (sk_metrics.classification_report(y_tr, y_hat_high_bias_tr, sample_weight=[{0:1, 1:5}[_y] for _y in y_tr]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance များတဲ့ model က \n",
    "\n",
    "* test performance ညံ့ပြီး\n",
    "* training performance ကောင်းနေပါလိမ့်မယ်။\n",
    "\n",
    "Bias များတဲ့ model က \n",
    "\n",
    "* test performance ရော\n",
    "* training performance ပါ ညံ့နေပါလိမ့်မယ္။"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Data to Have Cosine Distance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural Language Processing လို ကိစ္စမျိုးတွေမှာ cosine distance နဲ့မှ အဆင်ပြေတယ်။ \n",
    "\n",
    "ပြဿနာက `sklearn.cluster.KMeans` လိုမျိုး class တွေက cosine distance နဲ့ အလုပ်မလုပ်ဘူး။\n",
    "\n",
    "ဒီတော့ dataset ကို cosine distance နဲ့ တူအောင် ကိုယ့်ဟာကိုယ် လုပ်ယူရမယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster as sk_clus\n",
    "\n",
    "normalizer = sk_pp.Normalizer(norm=\"l2\")\n",
    "df_feat_tr_norm = normalizer.fit_transform(df_feat_tr)\n",
    "\n",
    "kmeans = sk_clus.KMeans()\n",
    "kmeans.fit(df_feat_tr_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Proof"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with definitions.\n",
    "\n",
    "Cosine can be derived from definition of dot-product:\n",
    "\n",
    "$X \\cdot Y = cos(\\theta)\\|X\\|\\|Y\\|$\n",
    "\n",
    "$\\Rightarrow cos(\\theta) = \\frac{X \\cdot Y}{\\|X\\|\\|Y\\|}$ \n",
    "\n",
    "> Also take note that dot-product $X \\cdot Y$ is a scalar and is the same as $X^T Y$\n",
    "\n",
    "When normalized, both $\\|X\\|$ and $\\|Y\\|$ becomes 1 $\\because$ it is the definition of normalization.\n",
    "\n",
    "$\\therefore cos(\\theta) = X \\cdot Y$ when both $X$ and $Y$ are normalized unit vectors.\n",
    "\n",
    "> The above $cos(\\theta)$ is cosine similarity because $cos(0)=1$ and $cos(\\pi/2)=0$.\n",
    ">\n",
    "> Cosine distance is given by $1 - cos(\\theta)$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Euclidean distance is L2-Norm of $X-Y$. \n",
    "\n",
    "$dist(X, Y) = \\|X - Y\\|$\n",
    "\n",
    "Let's square it.\n",
    "\n",
    "$dist(X, Y)^2 = \\|X - Y\\|^2$\n",
    "\n",
    "$= (X-Y)^T(X-Y)$\n",
    "\n",
    "> $ \\because V^T V = \\sum{v_i^2}$ by definition.\n",
    "\n",
    "$= (X^T-Y^T)(X-Y)$\n",
    "\n",
    "$= (X^T X) - (X^T Y) - (Y^T X) + (Y^T Y)$ \n",
    "\n",
    "> note that the two terms in middle are scaler and the same, i.e. $(X^T Y) = (Y^T X)$.\n",
    "\n",
    "$= (1) - (X^T Y) - (X^T Y) + (1)$\n",
    "\n",
    "$= 2 - 2 (X^T Y)$\n",
    "\n",
    "$= 2 (1 - X^T Y)$\n",
    "\n",
    "$= 2 (1 - X \\cdot Y)$\n",
    "\n",
    "$= 2 (1 - cos(\\theta))$\n",
    "\n",
    "$= 2 \\times$ cosine distance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37-dsup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
