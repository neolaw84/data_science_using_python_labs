{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models (Advanced)\n",
    "\n",
    "မင်္ဂလာပါ။ Data Science Using Python Week 14 က ကြိုဆိုပါတယ်။ \n",
    "\n",
    "ဒီတပါတ်မှာ linear model တွေအကြောင်းကို အသေးစိတ် ပြောမှာ ဖြစ်ပါတယ်။"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Linear Regression\n",
    "\n",
    "---\n",
    "\n",
    "### Recipe 10 - Linear Regression\n",
    "\n",
    "**Regression**: ဆိုတာ ဂဏန်းတန်ဖိုးတခုကို တွက်နည်းကို မသိပဲ အရင်က အဖြေတွေကို သိထားတဲ့အခါကျရင် နောက် unseen အဖြေတွေကို ခန့်မှန်း/တွက်ချက်ပေးမဲ့ function (Model) ကို တည်ဆောက်ခြင်းပါ။\n",
    "\n",
    "> လိုတာတွေကတော့ အရင်က $X$ (features/independent variables) နဲ့ အဖြေ $y$ (target/dependent variable) တွေပဲ။\n",
    "\n",
    "> validation/test dataset ကို သပ်သပ်ဖယ်ထားလေ့ရှိတယ်။\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model as sk_lm\n",
    "\n",
    "# initializing the model;\n",
    "linear_regression_model = sk_lm.LinearRegression()\n",
    "# building the model; \n",
    "linear_regression_model.fit(X, y)\n",
    "# using the model for unseen values\n",
    "y_pred = linear_regression_model.predict(X_unseen)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner-workings of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as sp_linalg, optimize as sp_optimize\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets, metrics, linear_model, model_selection as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = datasets.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(ds_y)\n",
    "k_select_every = 50\n",
    "selected_col = \"MedInc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = range(0, N, k_select_every)\n",
    "selected_X = df_X[selected_col][selector]\n",
    "selected_y = ds_y[selector]\n",
    "utils.scatter_xy(selected_X, y=selected_y, xlabel=selected_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LinearRegression()\n",
    "model.fit(selected_X.values.reshape((-1, 1)), selected_y)\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **အဲဒီ အပေါ်က ဂဏန်း ၂ လုံး ဘယ်က ရလာသလဲ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.scatter_xy(selected_X, y=selected_y, xlabel=selected_col)\n",
    "utils.draw_line(model.coef_[0], model.intercept_, ylim=(0, 5.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "လိုင်း အနီဟာ ဂဏန်း ၂ လုံးကို ကိုယ်စားပြုတယ်။ `model` ရဲ့ `predict` function ဟာ အောက်ပါ function နဲ့ တူတူပဲ ဖြစ်တယ်။\n",
    "\n",
    "```python\n",
    "def predict(x, coef_=model.coef_[0], intercept_=model.intercept_):\n",
    "    return x * coef_ + intercept_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, coef_=model.coef_[0], intercept_=model.intercept_):\n",
    "    return x * coef_ + intercept_\n",
    "predict(selected_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix this error\n",
    "temp_df = pd.DataFrame({\n",
    "    selected_col : selected_X, \n",
    "    \"predict_function\" : predict(selected_X),\n",
    "    \"model_predict_function\" : model.predict(selected_X.values.reshape((-1, 1))),\n",
    "})\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အဖြစ်မှန်နဲ့ `model` output နဲ့ ကွာခြားချက်ကို သိစေဖို့ `error` ကို `predict_function` ထဲက `selected_y` ကို နှုတ်ပြီး ရှာကြည့်ကြမယ်။ \n",
    "\n",
    "ပြီးတော့ အဲဒီ `error` ကို အမြဲ အပေါင်းကိန်းဖြစ်စေဖို့ ၂ ထပ် တင်လိုက်မယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df[\"error\"] = temp_df[\"predict_function\"] - selected_y\n",
    "temp_df[\"square_error\"] = temp_df[\"error\"].values ** 2\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sum_square_error(predict, actual):\n",
    "    return np.sum((predict - actual) ** 2)\n",
    "\n",
    "calculate_sum_square_error(temp_df[\"predict_function\"], selected_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "အခု `coef_` value 5 ခုနဲ့ `intercept_` value 5 ခုတို့ရဲ့ လိုင်းတွေကို ဆွဲကြည့်ပြီး သူတို့ရဲ့ sum_square_error ကို တွက်ကြည့်ကြစို့။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(nrows=5, ncols=5)\n",
    "error = np.zeros((5, 5))\n",
    "values = list(range(-2, +3, 1))\n",
    "for r_idx, (c, _plt_row) in enumerate(zip(values, ax)):\n",
    "    for c_idx, (i, _plt) in enumerate(zip(values, _plt_row)):\n",
    "        utils.draw_line(c, i, xlim=(-14, 14), ylim=(-5.5, 5.5), subplot=_plt)\n",
    "        error[r_idx, c_idx] = calculate_sum_square_error(\n",
    "            predict(selected_X, c, i),\n",
    "            selected_y\n",
    "        )\n",
    "plt.show()\n",
    "print (np.around(error, decimals=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the problem becomes:\n",
    "\n",
    "\n",
    "Given a set of bounds such that:\n",
    "\n",
    "$- \\infty \\leq \\text{coef} \\leq + \\infty$ and $- \\infty \\leq \\text{intercept} \\leq + \\infty$, \n",
    "\n",
    "to **minimize** a function $z(\\text{coef}, \\text{intercept})$ defined as: \n",
    "\n",
    "```python\n",
    "def z(_X):\n",
    "    c, i = _X\n",
    "    y_pred = predict(selected_X, coef_=c, intercept_=i)\n",
    "    return calculate_sum_square_error(y_pred, selected_y)\n",
    "```\n",
    "\n",
    "**Recipe 4 - Minimizing Non-Linear Functions** says, \n",
    "\n",
    "> `scipy.optimize` ထဲက `minimize` function ကိုသုံးပြီး ရှင်းပါ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z(_X):\n",
    "    c, i = _X\n",
    "    y_pred = predict(selected_X, coef_=c, intercept_=i)\n",
    "    return calculate_sum_square_error(y_pred, selected_y)\n",
    "\n",
    "bounds = ((-np.inf, np.inf), (-np.inf, np.inf))\n",
    "\n",
    "results = sp_optimize.minimize(fun=z, x0=[0.3, 0.3], bounds=bounds)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But, what about **Regularization** ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratio = 0.15\n",
    "def z_with_regularization(_X):\n",
    "    c, i = _X\n",
    "    y_pred = predict(selected_X, coef_=c, intercept_=i)\n",
    "    l1_weight = abs(c)\n",
    "    l2_weight = c ** 2\n",
    "    return calculate_sum_square_error(y_pred, selected_y) + l1_ratio * l1_weight + (1 - l1_ratio) * l2_weight\n",
    "\n",
    "bounds = ((-np.inf, np.inf), (-np.inf, np.inf))\n",
    "\n",
    "results = sp_optimize.minimize(fun=z_with_regularization, x0=[0.3, 0.3], bounds=bounds)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Logistic Regression\n",
    "\n",
    "---\n",
    "\n",
    "### Recipe 16 - **Classification** with Linear Models\n",
    "\n",
    "**Classification**: ဆိုတာ category (class) ခွဲနည်းကို မသိပဲ အရင်က အဖြေတွေကို သိထားတဲ့အခါကျရင် နောက် unseen အဖြေတွေကို ခန့်မှန်း/တွက်ချက်ပေးမဲ့ function (Model) ကို တည်ဆောက်ခြင်းဖြစ်တယ်။\n",
    "\n",
    "> လိုတာတွေကတော့ အရင်က $X$ (features/independent variables) နဲ့ အဖြေ $y$ (target/dependent variable) တွေပဲ။\n",
    "\n",
    "> validation/test dataset ကို သပ်သပ်ဖယ်ထားလေ့ရှိတယ်။\n",
    "\n",
    "```python\n",
    "from sklearn import linear_model as sk_lm\n",
    "\n",
    "# building the model; check closely these 3 lines\n",
    "linear_regression_model = sk_lm.LogisticRegression()\n",
    "linear_regression_model.fit(X, y)\n",
    "\n",
    "# for unseen values\n",
    "y_pred = linear_regression_model.predict(X_unseen)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner-workings of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X, ds_y = datasets.load_digits(n_class=2, return_X_y=True, as_frame=True)\n",
    "ds_y.astype(dtype=np.int8)\n",
    "ds_y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "selector = feature_selection.SelectKBest(score_func=feature_selection.f_classif, k=1)\n",
    "selector.fit(df_X.values, ds_y)\n",
    "features = selector.transform(df_X.values)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression()\n",
    "model.fit(features, ds_y)\n",
    "model.coef_, model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ပြဿနာ ၂ ခု \n",
    "\n",
    "1. **ပထမ ပြဿနာ** လိုချင်တာက 0 ဖြစ်မလား probability (or 1 ဖြစ်မလား probability)။ probability က 0 နဲ့ 1 ကြားထဲမှာပဲ ရှိတယ်။ \n",
    "   \n",
    "   Linear Regression လိုပဲ minimize လို့မရဘူး။ တချို့ တန်ဖိုးတွေက 0 ထက် ငယ်ပြီး တချို့က 1 ထက်ကြီးနေတယ်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.max(features), np.max(predict(features, model.coef_[0], model.intercept_)))\n",
    "print (np.min(features), np.min(predict(features, model.coef_[0], model.intercept_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ဒါကြောင့် ဒီနေရာမှာ $log(\\text{odd})$ ကို သုံးရတယ်။ \n",
    "\n",
    "    **odd** အကြောင်းလေး နည်းနည်း ရှင်းပြမယ်။ \n",
    "\n",
    "    အံစာတုံး တတုံးကို ပစ်လိုက်ရင် 3 ကျဖို့ probability ဘယ်လောက်ရှိသလဲ ? \n",
    "\n",
    "    $$p(X=3) = 1/6$$\n",
    "\n",
    "* စိတ်တွက်နဲ့တောင် ရတယ်။ \n",
    "\n",
    "    odd ဆိုတာကတော့ probability ကိုပဲ __ လေး __ လေး ပုံစံနဲ့ ပြောတာ ဖြစ်တယ်။ \n",
    "\n",
    "    **အံစာတုံး တတုံးပစ်လိုက်ရင် ၃ ကျဖို့ အခွင့်အလမ်းက ၁ လေး ၅ လေးပဲ ရှိတယ်။**\n",
    "\n",
    "    အံစာတုံး ပစ်လို့ စုံကိန်းကျဖို့ အခွင့်အလမ်းက ၁ လေး ၁ လေး (ဆတူ) ပဲ ရှိတယ်။ \n",
    "\n",
    "* ဖော်မြူလာအရတော့ ... \n",
    "\n",
    "    $$\\text{odd}(X=3) = \\frac{p(X=3)}{p(X\\neq3)} = \\frac{1/6}{5/6} = \\frac{1}{5}$$\n",
    "\n",
    "* **odd ရဲ့ ပြဿနာက balance မညီတာပဲ**\n",
    "\n",
    "    အံစာတုံး ပစ်လိုက်ရင် 3 ကျမဲ့ odd က 1/5 ဖြစ်ပေမဲ့ 3 မကျမဲ့ odd က 5/1 ဖြစ်နေတယ်။ \n",
    "\n",
    "    $$\\text{odd}(X=3) = \\frac{1}{25}\\text{odd}(X\\neq3)$$\n",
    "\n",
    "* အံစာတုံးပစ်လိုက်ရင် 1 သို့မဟုတ် 2 ကျဖို့ odd ကျတော့ 2/4 = 1/2။ 1 သို့မဟုတ် 2 မကျဖို့ odd ကျတော့ 4/2 = 2။ \n",
    "\n",
    "    အောက်က ဂရပ်ထဲမှာ ကြည့်ပါ။ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.ones(shape=(12))\n",
    "y = np.concatenate((x[:6] / np.arange(6), x[6:] * np.arange(6)))\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ဒီတော့ balance ညီသွားအောင် $log$ ယူလိုက်တယ်။\n",
    "\n",
    "$$log(\\text{odd}) = log (\\frac{p}{1-p})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, np.log(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "သတိထားဖို့က ... \n",
    "\n",
    "* $log(b^a) = alog(b)$\n",
    "* $log(ab) = log(a) + log(b)$\n",
    "* $log(\\frac{a}{b}) = log(a) - log(b)$\n",
    "* $log(1) = 0$ and $log(0)$ is undefined\n",
    "\n",
    "ဒီမှာ y axis ကို $log(\\text{odd})$ လို့ ပြောင်းလိုက်တဲ့အခါ Linear Regression လိုပဲ coef ဟာ $-\\infty$ ကနေ $+\\infty$ ဖြစ်သွားတယ်။ \n",
    "\n",
    "အဲဒီမှာ ဒုတိယ ပြဿနာနဲ့ တွေ့ရတယ်။"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **ဒုတိယ ပြဿနာ** error ကို တွက်မရတော့ဘူး။ Linear Regression နဲ့ မတူတာက ဒီမှာ true label တွေက probability တွေ ဖြစ်ပြီး 1 or 0 ပဲ ဖြစ်တယ်။ ဒီတော့ \n",
    "\n",
    "    * $p(X=1) = 0$ ဖြစ်တဲ့အခါကျ ... $\\text{odd}(X=1) = \\frac{p(X=1)}{1-p(X=1)} = \\frac{0}{1-0} = \\frac{0}{1}$ ဆိုတော့ ပြဿနာမရှိပေမဲ့\n",
    "    * $p(X=1) = 1$ ဖြစ်တဲ့အခါကျ ... $\\text{odd}(X=1) = \\frac{p(X=1)}{1-p(X=1)} = \\frac{1}{1-1} = \\frac{1}{0}$ ဆိုတော့ undefined ဖြစ်နေရော\n",
    "\n",
    "    > $P(X=0)$ အတွက်လဲ တူတူပဲ။\n",
    "\n",
    "    ဒီတော့ likelihood ကို maximize ဖို့ သုံးရတယ်။ \n",
    "\n",
    "    (အလွယ်နည်းပြောရရင်) predict function က ထုတ်ပေးလိုက်တဲ့ probability တွေကို စုပြုံ မြှောက်တာ likelihood ပဲ။ \n",
    "    \n",
    "    ဒီနေရာမှာ ဖောက်လာတာတွေက ... \n",
    "\n",
    "    * true $p(X=1) = 1.0$ ဖြစ်တဲ့အချိန်မှာ predicted $p(X=1)$ ကို probability အဖြစ် ယူရပြီး true $p(X=0) = 1.0$ ဖြစ်တဲ့ အချိန်မှာ predicted $p(X=0)$ ကို probability အဖြစ် ယူရတယ်။ \n",
    "\n",
    "    * ကွန်ပျူတာဟာ အပေါင်းကို အမြှောက်ထက် တွက်တာ ပိုတော်တယ်။ ဒီတော့ likelihood ကို maximize လုပ်မဲ့အစား log likelihood ကို maximize လုပ်ဖို့ ပြောင်းလိုက်တယ်။\n",
    "\n",
    "    * ထုံးစံအတိုင်း maximize တာက ပုံစံခွက်ထဲ မဝင်တာမို့ -1 နဲ့ မြှောက်လိုက်တယ်။ အဲဒီမှာ **negative log likelihood** ဆိုတာကြီးကို minimize ဖို့ ဖြစ်လာတယ်။ (အခေါ်အဝေါ်အားဖြင့်)\n",
    "\n",
    "    * အခေါ်အဝေါ်အားဖြင့် minimize ရတဲ့ term/function ကို loss လို့ ခေါ်ကြတယ်။ အဲဒါ NLLL ဆိုတာ အဲဒါပဲ (Negative Log Likelihood Loss)\n",
    "\n",
    "    * $log(\\text{odd}(X=1)) = log(\\frac{p}{1-p})$ ကို မှတ်မိတယ် မဟုတ်လား။ အဲဒါကြီးကနေ $p$ ရအောင် ပြန်တွက်တဲ့အခါကျ ... \n",
    "\n",
    "        $$p(X=1) = \\frac{e^{log(\\text{odd}(X=1))}}{1+e^{log(\\text{odd}(X=1))}}$$ \n",
    "\n",
    "        ဆိုပြီး ဖြစ်သွားတယ်။"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a9c1e998c7d6d5f29587b2c70e9bd488bb486b902354401efc27ca8457f04e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
