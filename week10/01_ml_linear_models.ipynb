{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning -- Linear Models\n",
    "\n",
    "မင်္ဂလာပါ၊ အခု week 10 မှာ Linear Models တွေကို လေ့လာကြမှာ ဖြစ်တယ်။\n",
    "\n",
    "![Data Science Venn Diagram](https://i.ibb.co/4NpbV0C/data-science.png)\n",
    "\n",
    "> ဒီအပါတ်ကစပြီး သင်္ချာပါဝင်မှုက အရင် အပါတ်တွေနဲ့စာရင် လျော့သွားတာကို တွေ့ရလိမ့်မယ်။\n",
    "\n",
    "> ဒါပေမဲ့ သုညအထိတော့ ပျောက်မသွားဘူး။\n",
    "\n",
    "**Machine Learning** ဆိုတာ **model** ဆောက်တာပဲလို့ အလွယ်ပြောလို့ရတယ်။ \n",
    "\n",
    "## **model** ကို ဘာအတွက် ဆောက်ကြတာလဲ၊ ဘယ်လို ဆောက်ကြတာလဲ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%git clone https://github.com/neolaw84/data_science_using_python_labs.git\n",
    "cd data_science_using_python_labs/week10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_boston_dataset\n",
    "df_boston = load_boston_dataset()\n",
    "df_boston.head()\n",
    "# up to this point is what we have seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple imports\n",
    "import pandas as pd\n",
    "from sklearn import model_selection as sk_ms\n",
    "from sklearn import linear_model as sk_lm\n",
    "from sklearn import preprocessing as sk_pr\n",
    "\n",
    "# we have seen values of DataFrame to get numpy arrays; nothing new\n",
    "X_bst = df_boston[[c for c in df_boston.columns if c != \"MEDV\"]].values\n",
    "y_bst = df_boston.MEDV.values\n",
    "\n",
    "# splitting data into train and test\n",
    "X_bst_tr, X_bst_ts, y_bst_tr, y_bst_ts = sk_ms.train_test_split(X_bst, y_bst, test_size=0.10)\n",
    "\n",
    "# building the model; check closely these 3 lines\n",
    "linear_regression_model = sk_lm.LinearRegression()\n",
    "linear_regression_model.fit(X_bst_tr, y_bst_tr)\n",
    "y_bst_pred = linear_regression_model.predict(X_bst_ts)\n",
    "\n",
    "# building the prediction dataframe for analysis\n",
    "df_bst_pred = pd.DataFrame({\"actual_medv\": y_bst_ts, \"pred_medv\": y_bst_pred, \"diff_medv\": y_bst_pred - y_bst_ts})\n",
    "df_bst_pred.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **model** တခုကို ဆောက်တယ်ဆိုတာ **function** တခုကို တည်ဆောက်တာပါပဲ။ တနည်းအားဖြင့် function တခုကို လိုချင်လို့ ဆောက်တာ ဖြစ်တယ်။\n",
    "\n",
    "* model တခုကို တည်ဆောက်ရာမှာ ...\n",
    "  * function ရဲ့ ***formula*** က အသေကြီး ရှိနေတယ်။ \n",
    "  * အဲဒါကိုလဲ **model** လို့ ခေါ်ပြီး \n",
    "  * သူ့ထဲထည့်ရတဲ့ **variable** ဂဏန်း အပြောင်းအလဲလေးတွေကို **model parameter** (`coef`) လို့ ခေါ်တယ်။ \n",
    "  * **performance** ကောင်းတဲ့ model တခုရလာဖို့ model parameter တွေကို ချိန်ညှိပေးရတယ်။\n",
    "  * ဒီလို ချိန်ညှိပေးတဲ့ algorithm ကို **training function** (`fit`) လို့ ခေါ်ပြီး \n",
    "  * training function ထဲ ထည့်ရတဲ့ parameter တွေကို **hyper parameter** လို့ ခေါ်တယ်။\n",
    "\n",
    "## model တခုရဲ့ performance ကောင်းတယ်၊ မကောင်းဘူးကို ဘယ်လိုတိုင်းမလဲ။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics as sk_met\n",
    "\n",
    "print (\"max error : \")\n",
    "print (sk_met.max_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean absolute error\")\n",
    "print (sk_met.mean_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"median absolute error\")\n",
    "print (sk_met.median_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean square error : \")\n",
    "print (sk_met.mean_squared_error(y_bst_ts, y_bst_pred))\n",
    "print (\"r^2 score : \")\n",
    "print (sk_met.r2_score(y_bst_ts, y_bst_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `max_error` ကတော့ အထွေအထူးရှင်းစရာ မရှိပါဘူး။ အများဆုံး error ပေါ့။\n",
    "\n",
    "* ဒါပေမဲ့ တချို့ error တွေက အနှုတ်ကိန်းတွေလဲ ရှိတယ်ဆိုတော့ အဲဒါတွေပါပါအောင် absolute error ဆိုတာ တွက်ကြတယ်။ အဲဒီ absolute error ကိုမှ `median_absolute_error` နဲ့ `mean_absolute_error` ဆက်တွက်လို့ ရတယ်။\n",
    "\n",
    "* `mean_square_error` ဆိုတာလဲ အနှုတ်ကိန်းကို ပျောက်အောင် လုပ်တဲ့သဘောပဲ။ ၂ ထပ်တင်လိုက်တာပေါ့။\n",
    "\n",
    "$$\n",
    "\\text{MSE}(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=0}^{m - 1} (y_i - \\hat{y}_i)^2.\n",
    "$$\n",
    "\n",
    "* အပေါ်က error metric တွေက နည်းလေ performance ကောင်းလေပဲ။ \n",
    "\n",
    "* `r2_score` ကျတော့ နည်းနည်းပိုရှင်းပြမှ ရမယ်။ \n",
    "\n",
    "  ပထမဆုံး predict လုပ်မဲ့ target `y` ကို mean နဲ့ ရမ်းရွှီးရင် ရမဲ့ $\\text{MSE}(y, \\bar{y})$ ကို အရင်ရှာရမယ်။\n",
    "\n",
    "  $$\\text{MSE}(y, \\bar{y}) = \\frac{1}{m}\\sum_{i=1}^{m} (y_i - \\bar{y})^2$$\n",
    "\n",
    "  ပြီးတော့ အဲဒါကို အခြေအဖြစ်တည်ပြီး $\\text{MSE}(y, \\hat{y})$ ကို သွားစားတယ် (normalize လုပ်တဲ့ သဘောပါပဲ)။ အဲဒီတော့ $\\frac{1}{m}$ တွေကျေသွားတယ်။\n",
    "\n",
    "  နောက်ဆုံး အဲဒီ normalize လုပ်ထား (စားထား) တဲ့ ratio ကို 1 ထဲက နှုတ်လိုက်တယ်။ \n",
    "\n",
    "  $$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{m} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "  **`r2_score` ဟာ 1 နဲ့ နီးလေ ကောင်းလေ၊ 1 ထက်နည်းလေ ညံ့လေပဲ။**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is new; check closely these 3 lines\n",
    "normalizer = sk_pr.Normalizer()\n",
    "normalizer.fit(X_bst, y_bst) # this does nothing; will explain later\n",
    "X_bst_norm = normalizer.transform(X_bst)\n",
    "\n",
    "# splitting data into train and test\n",
    "X_bst_tr_n, X_bst_ts_n, y_bst_tr, y_bst_ts = sk_ms.train_test_split(X_bst_norm, y_bst, test_size=0.10)\n",
    "\n",
    "# building the model; check closely these 3 lines\n",
    "linear_regression_model_2 = sk_lm.LinearRegression()\n",
    "linear_regression_model_2.fit(X_bst_tr_n, y_bst_tr)\n",
    "y_bst_pred = linear_regression_model_2.predict(X_bst_ts_n)\n",
    "\n",
    "# building the prediction dataframe for analysis\n",
    "df_bst_pred_2 = pd.DataFrame({\"actual_medv\": y_bst_ts, \"pred_medv\": y_bst_pred, \"diff_medv\": y_bst_pred - y_bst_ts})\n",
    "df_bst_pred_2.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"max error : \")\n",
    "print (sk_met.max_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean absolute error\")\n",
    "print (sk_met.mean_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"median absolute error\")\n",
    "print (sk_met.median_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean square error : \")\n",
    "print (sk_met.mean_squared_error(y_bst_ts, y_bst_pred))\n",
    "print (\"r^2 score : \")\n",
    "print (sk_met.r2_score(y_bst_ts, y_bst_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the model; check closely these 3 lines\n",
    "sgd_model = sk_lm.SGDRegressor(\n",
    "    penalty=\"elasticnet\", alpha=0.0001, l1_ratio=0.15, max_iter=1000, shuffle=True, random_state=42, learning_rate=\"invscaling\"\n",
    ")\n",
    "# penalty can be \"l1\", \"l2\" or \"elasticnet\"\n",
    "# learning rate can be 'constant', 'optimal', 'invscaling', 'adaptive'\n",
    "sgd_model.fit(X_bst_tr_n, y_bst_tr)\n",
    "y_bst_pred = sgd_model.predict(X_bst_ts_n)\n",
    "\n",
    "# building the prediction dataframe for analysis\n",
    "df_bst_pred_3 = pd.DataFrame({\"actual_medv\": y_bst_ts, \"pred_medv\": y_bst_pred, \"diff_medv\": y_bst_pred - y_bst_ts})\n",
    "df_bst_pred_3.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"max error : \")\n",
    "print (sk_met.max_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean absolute error\")\n",
    "print (sk_met.mean_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"median absolute error\")\n",
    "print (sk_met.median_absolute_error(y_bst_ts, y_bst_pred))\n",
    "print (\"mean square error : \")\n",
    "print (sk_met.mean_squared_error(y_bst_ts, y_bst_pred))\n",
    "print (\"r^2 score : \")\n",
    "print (sk_met.r2_score(y_bst_ts, y_bst_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets as sk_ds\n",
    "df_X_cal, df_y_cal = sk_ds.fetch_california_housing(return_X_y=True, as_frame=True)\n",
    "# build a model to predict df_y_cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, how to build models that can say \"Yes\" and \"No\" ?\n",
    "\n",
    "In the simplest form, we can use the same **regression** techniques to get a number $\\hat{y}$ between 0 and 1. Then decide as follow:\n",
    "\n",
    "$$f(X)=\\text{YES if } \\hat{y} \\geq 0.5 \\text{ else NO}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_wine, df_y_wine = sk_ds.load_wine(return_X_y=True, as_frame=True)\n",
    "# build a model to predict whether a wine is bad (grade-0) or good (grade-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "မတူညီတဲ့ class (category) တွေ output ထုတ်ပေးရတဲ့ model မျိုးကို **classification** model လို့ ခေါ်တယ်။ ပထမပိုင်းမှာ စမ်းသပ်ခဲ့တဲ့ norminal (numeric values) တွေ output ထုတ်ပေးရတဲ့ model မျိုးကို **regression** model လို့ ခေါ်တယ်။ \n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "နာမည်နဲ့ မလိုက်အောင်ပဲ Logistic Regression ဆိုတာ classification model ဖြစ်တယ်။ \n",
    "\n",
    "သူ့မှာ တခြားနာမည်တွေလဲ ရှိတယ်။\n",
    "\n",
    "* logit regression, \n",
    "* maximum-entropy classification (MaxEnt) or \n",
    "* the log-linear classifier\n",
    "\n",
    "အသေးစိတ် မပြောခင် အရင်စမ်းကြည့်ရအောင်။"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test\n",
    "X_wine_tr, X_wine_ts, y_wine_tr, y_wine_ts = sk_ms.train_test_split(df_X_wine, df_y_wine, test_size=0.10)\n",
    "lr_model_wine = sk_lm.LogisticRegression(penalty=\"elasticnet\", dual=False, C=1.0, random_state=42, max_iter=100, l1_ratio=0.15, solver=\"saga\", multi_class=\"ovr\")\n",
    "# penalty can be : 'l1', 'l2', 'elasticnet'\n",
    "# solver can be : 'liblinear', 'saga' etc. \n",
    "# multi_class can be : 'multinomial', 'auto' or 'ovr'\n",
    "lr_model_wine.fit(X_wine_tr, y_wine_tr)\n",
    "y_wine_pred = lr_model_wine.predict(X_wine_ts)\n",
    "\n",
    "# building the prediction dataframe for analysis\n",
    "df_wine_pred = pd.DataFrame({\"actual_y\": y_wine_ts, \"pred_y\": y_wine_pred})\n",
    "df_wine_pred.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_wine_pred_proba = lr_model_wine.predict_proba(X_wine_ts)\n",
    "y_wine_pred_proba[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_met.balanced_accuracy_score(y_wine_ts, y_wine_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_met.confusion_matrix(y_wine_ts, y_wine_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_met.roc_auc_score(y_wine_ts, y_wine_pred_proba, multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine_pred[\"binary_actual\"] = df_wine_pred.actual_y.apply(lambda x : 1 if x > 0 else 0)\n",
    "df_wine_pred[\"binary_pred\"] = df_wine_pred.pred_y.apply(lambda x : 1 if x > 0 else 0)\n",
    "df_wine_pred.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (sk_met.classification_report(df_wine_pred.binary_actual, df_wine_pred.binary_pred, labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sk_met.roc_curve(df_wine_pred.binary_actual, df_wine_pred.binary_pred, pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(fpr, tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2458648b5873a8511ce5d0b68ae5943af1ad0cfd25276a02b44cf6fbc34eca5c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37-dsup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
